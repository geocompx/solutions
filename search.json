[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"website provides solutions exercises Geocomputation R, 2nd edition.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"work licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"E1. Think terms ‘GIS’, ‘GDS’ ‘geocomputation’ described . () best describes work like using geo* methods software ?E2. Provide three reasons using scriptable language R geocomputation instead using graphical user interface (GUI) based GIS QGIS.E3. year 2000 Stan Openshaw wrote geocomputation involved “practical work beneficial useful” others. Think practical problem possible solutions informed new evidence derived analysis, visualization modelling geographic data. pen paper (computational equivalent) sketch inputs possible outputs illustrating geocomputation help.","code":""},{"path":"spatial-class.html","id":"spatial-class","chapter":"2 Geographic data in R","heading":"2 Geographic data in R","text":"E1. Use summary() geometry column world data object included spData package. output tell us :geometry type?number countries?coordinate reference system (CRS)?E2. Run code ‘generated’ map world Section 2.2.3 (Basic map making).\nFind two similarities two differences image computer book.cex argument (see ?plot)?cex set sqrt(world$pop) / 10000?Bonus: experiment different ways visualize global population.E3. Use plot() create maps Nigeria context (see Section 2.2.3).Adjust lwd, col expandBB arguments plot().Challenge: read documentation text() annotate map.E4. Create empty SpatRaster object called my_raster 10 columns 10 rows.\nAssign random values 0 10 new raster plot .E5. Read-raster/nlcd.tif file spDataLarge package.\nkind information can get properties file?E6. Check CRS raster/nlcd.tif file spDataLarge package.\nkind information can learn ?WKT describes two-dimensional projected coordinate reference system.\nbased GRS 1980 ellipsoid North American Datum 1983 Greenwich prime meridian.\nused Transverse Mercator projection transform geographic projected CRS (UTM zone 12N).\nfirst axis related eastness, second one related northness, axes units meters.\nSRID CRS “EPSG:26912”.","code":"\nlibrary(sf)\nlibrary(spData)\nlibrary(terra)\nsummary(world)\n#>     iso_a2           name_long          continent          region_un        \n#>  Length:177         Length:177         Length:177         Length:177        \n#>  Class :character   Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>                                                                             \n#>   subregion             type              area_km2             pop          \n#>  Length:177         Length:177         Min.   :    2417   Min.   :5.63e+04  \n#>  Class :character   Class :character   1st Qu.:   46185   1st Qu.:3.75e+06  \n#>  Mode  :character   Mode  :character   Median :  185004   Median :1.04e+07  \n#>                                        Mean   :  832558   Mean   :4.28e+07  \n#>                                        3rd Qu.:  621860   3rd Qu.:3.07e+07  \n#>                                        Max.   :17018507   Max.   :1.36e+09  \n#>                                                           NA's   :10        \n#>     lifeExp       gdpPercap                 geom    \n#>  Min.   :50.6   Min.   :   597   MULTIPOLYGON :177  \n#>  1st Qu.:65.0   1st Qu.:  3752   epsg:4326    :  0  \n#>  Median :72.9   Median : 10734   +proj=long...:  0  \n#>  Mean   :70.9   Mean   : 17106                      \n#>  3rd Qu.:76.8   3rd Qu.: 24233                      \n#>  Max.   :83.6   Max.   :120860                      \n#>  NA's   :10     NA's   :17\n# - Its geometry type?\n#   multipolygon\n# - The number of countries?\n#   177\n# - Its coordinate reference system (CRS)?\n#   epsg:4326\nplot(world[\"continent\"], reset = FALSE)\ncex = sqrt(world$pop) / 10000\nworld_cents = st_centroid(world, of_largest = TRUE)\n#> Warning: st_centroid assumes attributes are constant over geometries\nplot(st_geometry(world_cents), add = TRUE, cex = cex)\n# - What does the `cex` argument do (see `?plot`)?\n#   It specifies the size of the circles\n# - Why was `cex` set to the `sqrt(world$pop) / 10000`?\n#   So the circles would be visible for small countries but not too large for large countries, also because area increases as a linear function of the square route of the diameter defined by `cex`\n# - Bonus: experiment with different ways to visualize the global population.\nplot(st_geometry(world_cents), cex = world$pop / 1e9)\nplot(st_geometry(world_cents), cex = world$pop / 1e8)\nplot(world[\"pop\"])\nplot(world[\"pop\"], logz = TRUE)\n\n# Similarities: global extent, colorscheme, relative size of circles\n# \n# Differences: projection (Antarctica is much smaller for example), graticules, location of points in the countries.\n# \n# To understand these differences read-over, run, and experiment with different argument values in this script: https://github.com/geocompx/geocompr/raw/main/code/02-contpop.R\n# \n# `cex` refers to the diameter of symbols plotted, as explained by the help page `?graphics::points`. It is an acronym for 'Chacter symbol EXpansion'.\n# It was set to the square route of the population divided by 10,000 because a) otherwise the symbols would not fit on the map and b) to make circle area proportional to population.\nnigeria = world[world$name_long == \"Nigeria\", ]\nplot(st_geometry(nigeria), expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(world[0], add = TRUE)\nworld_coords = st_coordinates(world_cents)\ntext(world_coords, world$iso_a2)\n\n# Alternative answer:\nnigeria = world[world$name_long == \"Nigeria\", ]\nafrica = world[world$continent == \"Africa\", ]\nplot(st_geometry(nigeria), col = \"white\", lwd = 3, main = \"Nigeria in context\", border = \"lightgrey\", expandBB = c(0.5, 0.2, 0.5, 0.2))\nplot(st_geometry(world), lty = 3, add = TRUE, border = \"grey\")\nplot(st_geometry(nigeria), col = \"yellow\", add = TRUE, border = \"darkgrey\")\na = africa[grepl(\"Niger\", africa$name_long), ]\nncentre = st_centroid(a)\n#> Warning: st_centroid assumes attributes are constant over geometries\nncentre_num = st_coordinates(ncentre)\ntext(x = ncentre_num[, 1], y = ncentre_num[, 2], labels = a$name_long)\nmy_raster = rast(ncol = 10, nrow = 10,\n                 vals = sample(0:10, size = 10 * 10, replace = TRUE))\nplot(my_raster)\nnlcd = rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\ndim(nlcd) # dimensions\n#> [1] 1359 1073    1\nres(nlcd) # resolution\n#> [1] 31.5 31.5\next(nlcd) # extent\n#> SpatExtent : 301903.344386758, 335735.354381954, 4111244.46098842, 4154086.47216415 (xmin, xmax, ymin, ymax)\nnlyr(nlcd) # number of layers\n#> [1] 1\ncat(crs(nlcd)) # CRS\n#> PROJCRS[\"NAD83 / UTM zone 12N\",\n#>     BASEGEOGCRS[\"NAD83\",\n#>         DATUM[\"North American Datum 1983\",\n#>             ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>                 LENGTHUNIT[\"metre\",1]]],\n#>         PRIMEM[\"Greenwich\",0,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         ID[\"EPSG\",4269]],\n#>     CONVERSION[\"UTM zone 12N\",\n#>         METHOD[\"Transverse Mercator\",\n#>             ID[\"EPSG\",9807]],\n#>         PARAMETER[\"Latitude of natural origin\",0,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8801]],\n#>         PARAMETER[\"Longitude of natural origin\",-111,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8802]],\n#>         PARAMETER[\"Scale factor at natural origin\",0.9996,\n#>             SCALEUNIT[\"unity\",1],\n#>             ID[\"EPSG\",8805]],\n#>         PARAMETER[\"False easting\",500000,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8806]],\n#>         PARAMETER[\"False northing\",0,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8807]]],\n#>     CS[Cartesian,2],\n#>         AXIS[\"(E)\",east,\n#>             ORDER[1],\n#>             LENGTHUNIT[\"metre\",1]],\n#>         AXIS[\"(N)\",north,\n#>             ORDER[2],\n#>             LENGTHUNIT[\"metre\",1]],\n#>     USAGE[\n#>         SCOPE[\"Engineering survey, topographic mapping.\"],\n#>         AREA[\"North America - between 114°W and 108°W - onshore and offshore. Canada - Alberta; Northwest Territories; Nunavut; Saskatchewan. United States (USA) - Arizona; Colorado; Idaho; Montana; New Mexico; Utah; Wyoming.\"],\n#>         BBOX[31.33,-114,84,-108]],\n#>     ID[\"EPSG\",26912]]\ncat(crs(nlcd))\n#> PROJCRS[\"NAD83 / UTM zone 12N\",\n#>     BASEGEOGCRS[\"NAD83\",\n#>         DATUM[\"North American Datum 1983\",\n#>             ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>                 LENGTHUNIT[\"metre\",1]]],\n#>         PRIMEM[\"Greenwich\",0,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         ID[\"EPSG\",4269]],\n#>     CONVERSION[\"UTM zone 12N\",\n#>         METHOD[\"Transverse Mercator\",\n#>             ID[\"EPSG\",9807]],\n#>         PARAMETER[\"Latitude of natural origin\",0,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8801]],\n#>         PARAMETER[\"Longitude of natural origin\",-111,\n#>             ANGLEUNIT[\"degree\",0.0174532925199433],\n#>             ID[\"EPSG\",8802]],\n#>         PARAMETER[\"Scale factor at natural origin\",0.9996,\n#>             SCALEUNIT[\"unity\",1],\n#>             ID[\"EPSG\",8805]],\n#>         PARAMETER[\"False easting\",500000,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8806]],\n#>         PARAMETER[\"False northing\",0,\n#>             LENGTHUNIT[\"metre\",1],\n#>             ID[\"EPSG\",8807]]],\n#>     CS[Cartesian,2],\n#>         AXIS[\"(E)\",east,\n#>             ORDER[1],\n#>             LENGTHUNIT[\"metre\",1]],\n#>         AXIS[\"(N)\",north,\n#>             ORDER[2],\n#>             LENGTHUNIT[\"metre\",1]],\n#>     USAGE[\n#>         SCOPE[\"Engineering survey, topographic mapping.\"],\n#>         AREA[\"North America - between 114°W and 108°W - onshore and offshore. Canada - Alberta; Northwest Territories; Nunavut; Saskatchewan. United States (USA) - Arizona; Colorado; Idaho; Montana; New Mexico; Utah; Wyoming.\"],\n#>         BBOX[31.33,-114,84,-108]],\n#>     ID[\"EPSG\",26912]]"},{"path":"attr.html","id":"attr","chapter":"3 Attribute data operations","heading":"3 Attribute data operations","text":"exercises use us_states us_states_df datasets spData package.\nmust attached package, packages used attribute operations chapter (sf, dplyr, terra) commands library(spData) attempting exercises:us_states spatial object (class sf), containing geometry attributes (including name, region, area, population) states within contiguous United States.\nus_states_df data frame (class data.frame) containing name additional variables (including median income poverty level, years 2010 2015) US states, including Alaska, Hawaii Puerto Rico.\ndata comes United States Census Bureau, documented ?us_states ?us_states_df.E1. Create new object called us_states_name contains NAME column us_states object using either base R ([) tidyverse (select()) syntax.\nclass new object makes geographic?class sf data.frame: 2 classes.sf class makes geographic.specifically attributes object (sf_column) geometry column (bbox, crs) make geographic.E2. Select columns us_states object contain population data.\nObtain result using different command (bonus: try find three ways obtaining result).\nHint: try use helper functions, contains matches dplyr (see ?contains).E3. Find states following characteristics (bonus find plot ):Belong Midwest region.Belong West region, area 250,000 km2and 2015 population greater 5,000,000 residents (hint: may need use function units::set_units() .numeric()).Belong South region, area larger 150,000 km2 total population 2015 larger 7,000,000 residents.E4. total population 2015 us_states dataset?\nminimum maximum total population 2015?E5. many states region?E6. minimum maximum total population 2015 region?\ntotal population 2015 region?E7. Add variables us_states_df us_states, create new object called us_states_stats.\nfunction use ?\nvariable key datasets?\nclass new object?E8. us_states_df two rows us_states.\ncan find ? (hint: try use dplyr::anti_join() function)E9. population density 2015 state?\npopulation density 2010 state?E10. much population density changed 2010 2015 state?\nCalculate change percentages map .E11. Change columns’ names us_states lowercase. (Hint: helper functions - tolower() colnames() may help.)E12. Using us_states us_states_df create new object called us_states_sel.\nnew object two variables - median_income_15 geometry.\nChange name median_income_15 column Income.E13. Calculate change number residents living poverty level 2010 2015 state. (Hint: See ?us_states_df documentation poverty level columns.)\nBonus: Calculate change percentage residents living poverty level state.E14. minimum, average maximum state’s number people living poverty line 2015 region?\nBonus: region largest increase people living poverty line?E15. Create raster scratch nine rows columns resolution 0.5 decimal degrees (WGS84).\nFill random numbers.\nExtract values four corner cells.E16. common class example raster grain?E17. Plot histogram boxplot dem.tif file spDataLarge package (system.file(\"raster/dem.tif\", package = \"spDataLarge\")).","code":"\nlibrary(sf)\nlibrary(dplyr)\nlibrary(terra)\nlibrary(spData)\ndata(us_states)\ndata(us_states_df)\nus_states_name = us_states[\"NAME\"]\nclass(us_states_name)\n#> [1] \"sf\"         \"data.frame\"\nattributes(us_states_name)\n#> $names\n#> [1] \"NAME\"     \"geometry\"\n#> \n#> $row.names\n#>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n#> [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n#> \n#> $class\n#> [1] \"sf\"         \"data.frame\"\n#> \n#> $sf_column\n#> [1] \"geometry\"\n#> \n#> $agr\n#> NAME \n#> <NA> \n#> Levels: constant aggregate identity\nattributes(us_states_name$geometry)\n#> $n_empty\n#> [1] 0\n#> \n#> $crs\n#> Coordinate Reference System:\n#>   User input: EPSG:4269 \n#>   wkt:\n#> GEOGCS[\"NAD83\",\n#>     DATUM[\"North_American_Datum_1983\",\n#>         SPHEROID[\"GRS 1980\",6378137,298.257222101,\n#>             AUTHORITY[\"EPSG\",\"7019\"]],\n#>         TOWGS84[0,0,0,0,0,0,0],\n#>         AUTHORITY[\"EPSG\",\"6269\"]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         AUTHORITY[\"EPSG\",\"8901\"]],\n#>     UNIT[\"degree\",0.0174532925199433,\n#>         AUTHORITY[\"EPSG\",\"9122\"]],\n#>     AUTHORITY[\"EPSG\",\"4269\"]]\n#> \n#> $class\n#> [1] \"sfc_MULTIPOLYGON\" \"sfc\"             \n#> \n#> $precision\n#> [1] 0\n#> \n#> $bbox\n#>   xmin   ymin   xmax   ymax \n#> -124.7   24.6  -67.0   49.4\nus_states |> select(total_pop_10, total_pop_15)\n#> Simple feature collection with 49 features and 2 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    total_pop_10 total_pop_15                       geometry\n#> 1       4712651      4830620 MULTIPOLYGON (((-88.2 35, -...\n#> 2       6246816      6641928 MULTIPOLYGON (((-115 32.7, ...\n#> 3       4887061      5278906 MULTIPOLYGON (((-109 41, -1...\n#> 4       3545837      3593222 MULTIPOLYGON (((-73.5 42, -...\n#> 5      18511620     19645772 MULTIPOLYGON (((-81.8 24.6,...\n#> 6       9468815     10006693 MULTIPOLYGON (((-85.6 35, -...\n#> 7       1526797      1616547 MULTIPOLYGON (((-117 46, -1...\n#> 8       6417398      6568645 MULTIPOLYGON (((-87.5 41.7,...\n#> 9       2809329      2892987 MULTIPOLYGON (((-102 40, -1...\n#> 10      4429940      4625253 MULTIPOLYGON (((-92 29.6, -...\n\n# or\nus_states |> select(starts_with(\"total_pop\"))\n#> Simple feature collection with 49 features and 2 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    total_pop_10 total_pop_15                       geometry\n#> 1       4712651      4830620 MULTIPOLYGON (((-88.2 35, -...\n#> 2       6246816      6641928 MULTIPOLYGON (((-115 32.7, ...\n#> 3       4887061      5278906 MULTIPOLYGON (((-109 41, -1...\n#> 4       3545837      3593222 MULTIPOLYGON (((-73.5 42, -...\n#> 5      18511620     19645772 MULTIPOLYGON (((-81.8 24.6,...\n#> 6       9468815     10006693 MULTIPOLYGON (((-85.6 35, -...\n#> 7       1526797      1616547 MULTIPOLYGON (((-117 46, -1...\n#> 8       6417398      6568645 MULTIPOLYGON (((-87.5 41.7,...\n#> 9       2809329      2892987 MULTIPOLYGON (((-102 40, -1...\n#> 10      4429940      4625253 MULTIPOLYGON (((-92 29.6, -...\n\n# or\nus_states |> select(contains(\"total_pop\"))\n#> Simple feature collection with 49 features and 2 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    total_pop_10 total_pop_15                       geometry\n#> 1       4712651      4830620 MULTIPOLYGON (((-88.2 35, -...\n#> 2       6246816      6641928 MULTIPOLYGON (((-115 32.7, ...\n#> 3       4887061      5278906 MULTIPOLYGON (((-109 41, -1...\n#> 4       3545837      3593222 MULTIPOLYGON (((-73.5 42, -...\n#> 5      18511620     19645772 MULTIPOLYGON (((-81.8 24.6,...\n#> 6       9468815     10006693 MULTIPOLYGON (((-85.6 35, -...\n#> 7       1526797      1616547 MULTIPOLYGON (((-117 46, -1...\n#> 8       6417398      6568645 MULTIPOLYGON (((-87.5 41.7,...\n#> 9       2809329      2892987 MULTIPOLYGON (((-102 40, -1...\n#> 10      4429940      4625253 MULTIPOLYGON (((-92 29.6, -...\n\n# or\nus_states |> select(matches(\"tal_p\"))\n#> Simple feature collection with 49 features and 2 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    total_pop_10 total_pop_15                       geometry\n#> 1       4712651      4830620 MULTIPOLYGON (((-88.2 35, -...\n#> 2       6246816      6641928 MULTIPOLYGON (((-115 32.7, ...\n#> 3       4887061      5278906 MULTIPOLYGON (((-109 41, -1...\n#> 4       3545837      3593222 MULTIPOLYGON (((-73.5 42, -...\n#> 5      18511620     19645772 MULTIPOLYGON (((-81.8 24.6,...\n#> 6       9468815     10006693 MULTIPOLYGON (((-85.6 35, -...\n#> 7       1526797      1616547 MULTIPOLYGON (((-117 46, -1...\n#> 8       6417398      6568645 MULTIPOLYGON (((-87.5 41.7,...\n#> 9       2809329      2892987 MULTIPOLYGON (((-102 40, -1...\n#> 10      4429940      4625253 MULTIPOLYGON (((-92 29.6, -...\nus_states |> \n  filter(REGION == \"Midwest\")\n#> Simple feature collection with 12 features and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -104 ymin: 36 xmax: -80.5 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    GEOID         NAME  REGION          AREA total_pop_10 total_pop_15\n#> 1     18      Indiana Midwest  93648 [km^2]      6417398      6568645\n#> 2     20       Kansas Midwest 213037 [km^2]      2809329      2892987\n#> 3     27    Minnesota Midwest 218566 [km^2]      5241914      5419171\n#> 4     29     Missouri Midwest 180716 [km^2]      5922314      6045448\n#> 5     38 North Dakota Midwest 183178 [km^2]       659858       721640\n#> 6     46 South Dakota Midwest 199767 [km^2]       799462       843190\n#> 7     17     Illinois Midwest 145993 [km^2]     12745359     12873761\n#> 8     19         Iowa Midwest 145744 [km^2]      3016267      3093526\n#> 9     26     Michigan Midwest 151119 [km^2]      9952687      9900571\n#> 10    31     Nebraska Midwest 200272 [km^2]      1799125      1869365\n#>                          geometry\n#> 1  MULTIPOLYGON (((-87.5 41.7,...\n#> 2  MULTIPOLYGON (((-102 40, -1...\n#> 3  MULTIPOLYGON (((-97.2 49, -...\n#> 4  MULTIPOLYGON (((-95.8 40.6,...\n#> 5  MULTIPOLYGON (((-104 49, -1...\n#> 6  MULTIPOLYGON (((-104 45, -1...\n#> 7  MULTIPOLYGON (((-91.4 40.4,...\n#> 8  MULTIPOLYGON (((-96.5 43.5,...\n#> 9  MULTIPOLYGON (((-85.6 45.6,...\n#> 10 MULTIPOLYGON (((-104 43, -1...\n\nus_states |> filter(REGION == \"West\", AREA < units::set_units(250000, km^2), total_pop_15 > 5000000)\n#> Simple feature collection with 1 feature and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 45.5 xmax: -117 ymax: 49\n#> Geodetic CRS:  NAD83\n#>   GEOID       NAME REGION          AREA total_pop_10 total_pop_15\n#> 1    53 Washington   West 175436 [km^2]      6561297      6985464\n#>                         geometry\n#> 1 MULTIPOLYGON (((-123 48.2, ...\n# or\nus_states |> filter(REGION == \"West\", as.numeric(AREA) < 250000, total_pop_15 > 5000000)\n#> Simple feature collection with 1 feature and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 45.5 xmax: -117 ymax: 49\n#> Geodetic CRS:  NAD83\n#>   GEOID       NAME REGION          AREA total_pop_10 total_pop_15\n#> 1    53 Washington   West 175436 [km^2]      6561297      6985464\n#>                         geometry\n#> 1 MULTIPOLYGON (((-123 48.2, ...\n\nus_states |> filter(REGION == \"South\", AREA > units::set_units(150000, km^2), total_pop_15 > 7000000)\n#> Simple feature collection with 3 features and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -107 ymin: 24.6 xmax: -80 ymax: 36.5\n#> Geodetic CRS:  NAD83\n#>   GEOID    NAME REGION          AREA total_pop_10 total_pop_15\n#> 1    12 Florida  South 151052 [km^2]     18511620     19645772\n#> 2    13 Georgia  South 152725 [km^2]      9468815     10006693\n#> 3    48   Texas  South 687714 [km^2]     24311891     26538614\n#>                         geometry\n#> 1 MULTIPOLYGON (((-81.8 24.6,...\n#> 2 MULTIPOLYGON (((-85.6 35, -...\n#> 3 MULTIPOLYGON (((-103 36.5, ...\n# or\nus_states |> filter(REGION == \"South\", as.numeric(AREA) > 150000, total_pop_15 > 7000000)\n#> Simple feature collection with 3 features and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -107 ymin: 24.6 xmax: -80 ymax: 36.5\n#> Geodetic CRS:  NAD83\n#>   GEOID    NAME REGION          AREA total_pop_10 total_pop_15\n#> 1    12 Florida  South 151052 [km^2]     18511620     19645772\n#> 2    13 Georgia  South 152725 [km^2]      9468815     10006693\n#> 3    48   Texas  South 687714 [km^2]     24311891     26538614\n#>                         geometry\n#> 1 MULTIPOLYGON (((-81.8 24.6,...\n#> 2 MULTIPOLYGON (((-85.6 35, -...\n#> 3 MULTIPOLYGON (((-103 36.5, ...\nus_states |> summarize(total_pop = sum(total_pop_15),\n                        min_pop = min(total_pop_15),\n                        max_pop = max(total_pop_15))\n#> Simple feature collection with 1 feature and 3 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#>   total_pop min_pop  max_pop                       geometry\n#> 1  3.14e+08  579679 38421464 MULTIPOLYGON (((-123 48.2, ...\nus_states |>\n  group_by(REGION) |>\n  summarize(nr_of_states = n())\n#> Simple feature collection with 4 features and 2 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> # A tibble: 4 × 3\n#>   REGION   nr_of_states                                                 geometry\n#>   <fct>           <int>                                       <MULTIPOLYGON [°]>\n#> 1 Norteast            9 (((-70.8 42.9, -70.7 43.1, -70.6 43.1, -70.5 43.3, -70.…\n#> 2 Midwest            12 (((-85.5 45.6, -85.5 45.8, -85.6 45.8, -85.6 45.6, -85.…\n#> 3 South              17 (((-81.4 30.7, -81.4 30.8, -81.4 30.9, -81.4 31, -81.3 …\n#> 4 West               11 (((-118 33.4, -118 33.3, -118 33.3, -118 33.4, -119 33.…\nus_states |>\n  group_by(REGION) |>\n  summarize(min_pop = min(total_pop_15),\n            max_pop = max(total_pop_15),\n            tot_pop = sum(total_pop_15))\n#> Simple feature collection with 4 features and 4 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> # A tibble: 4 × 5\n#>   REGION   min_pop  max_pop   tot_pop                                   geometry\n#>   <fct>      <dbl>    <dbl>     <dbl>                         <MULTIPOLYGON [°]>\n#> 1 Norteast  626604 19673174  55989520 (((-70.8 42.9, -70.7 43.1, -70.6 43.1, -7…\n#> 2 Midwest   721640 12873761  67546398 (((-85.5 45.6, -85.5 45.8, -85.6 45.8, -8…\n#> 3 South     647484 26538614 118575377 (((-81.4 30.7, -81.4 30.8, -81.4 30.9, -8…\n#> 4 West      579679 38421464  72264052 (((-118 33.4, -118 33.3, -118 33.3, -118 …\nus_states_stats = us_states |>\n  left_join(us_states_df, by = c(\"NAME\" = \"state\"))\nclass(us_states_stats)\n#> [1] \"sf\"         \"data.frame\"\nus_states_df |>\n  anti_join(st_drop_geometry(us_states), by = c(\"state\" = \"NAME\"))\n#> # A tibble: 2 × 5\n#>   state  median_income_10 median_income_15 poverty_level_10 poverty_level_15\n#>   <chr>             <dbl>            <dbl>            <dbl>            <dbl>\n#> 1 Alaska            29509            31455            64245            72957\n#> 2 Hawaii            29945            31051           124627           153944\nus_states2 = us_states |>\n  mutate(pop_dens_15 = total_pop_15/AREA,\n         pop_dens_10 = total_pop_10/AREA)\nus_popdens_change = us_states2 |>\n  mutate(pop_dens_diff_10_15 = pop_dens_15 - pop_dens_10,\n         pop_dens_diff_10_15p = (pop_dens_diff_10_15/pop_dens_15) * 100)\nplot(us_popdens_change[\"pop_dens_diff_10_15p\"])\nus_states %>%\n  setNames(tolower(colnames(.)))\n#> Simple feature collection with 49 features and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -125 ymin: 24.6 xmax: -67 ymax: 49.4\n#> Geodetic CRS:  NAD83\n#> First 10 features:\n#>    geoid        name   region          area total_pop_10 total_pop_15\n#> 1     01     Alabama    South 133709 [km^2]      4712651      4830620\n#> 2     04     Arizona     West 295281 [km^2]      6246816      6641928\n#> 3     08    Colorado     West 269573 [km^2]      4887061      5278906\n#> 4     09 Connecticut Norteast  12977 [km^2]      3545837      3593222\n#> 5     12     Florida    South 151052 [km^2]     18511620     19645772\n#> 6     13     Georgia    South 152725 [km^2]      9468815     10006693\n#> 7     16       Idaho     West 216513 [km^2]      1526797      1616547\n#> 8     18     Indiana  Midwest  93648 [km^2]      6417398      6568645\n#> 9     20      Kansas  Midwest 213037 [km^2]      2809329      2892987\n#> 10    22   Louisiana    South 122346 [km^2]      4429940      4625253\n#>                          geometry\n#> 1  MULTIPOLYGON (((-88.2 35, -...\n#> 2  MULTIPOLYGON (((-115 32.7, ...\n#> 3  MULTIPOLYGON (((-109 41, -1...\n#> 4  MULTIPOLYGON (((-73.5 42, -...\n#> 5  MULTIPOLYGON (((-81.8 24.6,...\n#> 6  MULTIPOLYGON (((-85.6 35, -...\n#> 7  MULTIPOLYGON (((-117 46, -1...\n#> 8  MULTIPOLYGON (((-87.5 41.7,...\n#> 9  MULTIPOLYGON (((-102 40, -1...\n#> 10 MULTIPOLYGON (((-92 29.6, -...\nus_states_sel = us_states |>\n  left_join(us_states_df, by = c(\"NAME\" = \"state\")) |>\n  select(Income = median_income_15)\nus_pov_change = us_states |>\n  left_join(us_states_df, by = c(\"NAME\" = \"state\")) |>\n  mutate(pov_change = poverty_level_15 - poverty_level_10)\n \n# Bonus\nus_pov_pct_change = us_states |>\n  left_join(us_states_df, by = c(\"NAME\" = \"state\")) |>\n  mutate(pov_pct_10 = (poverty_level_10 / total_pop_10) * 100, \n         pov_pct_15 = (poverty_level_15 / total_pop_15) * 100) |>\n  mutate(pov_pct_change = pov_pct_15 - pov_pct_10)\nus_pov_change_reg = us_pov_change |>\n  group_by(REGION) |>\n  summarize(min_state_pov_15 = min(poverty_level_15),\n            mean_state_pov_15 = mean(poverty_level_15),\n            max_state_pov_15 = max(poverty_level_15))\n\n# Bonus\nus_pov_change |>\n  group_by(REGION) |>\n  summarize(region_pov_change = sum(pov_change)) |>\n  filter(region_pov_change == max(region_pov_change)) |>\n  pull(REGION) |>\n  as.character()\n#> [1] \"South\"\nr = rast(nrow = 9, ncol = 9, res = 0.5,\n         xmin = 0, xmax = 4.5, ymin = 0, ymax = 4.5,\n         vals = rnorm(81))\n# using cell IDs\nr[c(1, 9, 81 - 9 + 1, 81)]\n#>    lyr.1\n#> 1  1.434\n#> 2 -0.265\n#> 3 -0.587\n#> 4 -2.593\nr[c(1, nrow(r)), c(1, ncol(r))]\n#>    lyr.1\n#> 1  1.434\n#> 2 -0.265\n#> 3 -0.587\n#> 4 -2.593\ngrain = rast(system.file(\"raster/grain.tif\", package = \"spData\"))\nfreq(grain) |> \n  arrange(-count )# the most common classes are silt and sand (13 cells)\n#>   layer value count\n#> 1     1  silt    13\n#> 2     1  sand    13\n#> 3     1  clay    10\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nhist(dem)\nboxplot(dem)\n\n# we can also use ggplot2 after converting SpatRaster to a data frame\nlibrary(ggplot2)\nggplot(as.data.frame(dem), aes(dem)) + geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot(as.data.frame(dem), aes(dem)) + geom_boxplot()"},{"path":"spatial-operations.html","id":"spatial-operations","chapter":"4 Spatial data operations","heading":"4 Spatial data operations","text":"E1. established Section ?? Canterbury region New Zealand containing 100 highest points country.\nmany high points Canterbury region contain?Bonus: plot result using plot() function show New Zealand, canterbury region highlighted yellow, high points Canterbury represented red crosses (hint: pch = 7) high points parts New Zealand represented blue circles. See help page ?points details illustration different pch values.E2. region second highest number nz_height points, many ?E3. Generalizing question regions: many New Zealand’s 16 regions contain points belong top 100 highest points country? regions?Bonus: create table listing regions order number points name.E4. Test knowledge spatial predicates finding plotting US states relate spatial objects.starting point exercise create object representing Colorado state USA. command\ncolorado = us_states[us_states$NAME == \"Colorado\",] (base R) filter() function (tidyverse) plot resulting object context US states.Create new object representing states geographically intersect Colorado plot result (hint: concise way subsetting method [).Create another object representing objects touch (shared boundary ) Colorado plot result (hint: remember can use argument op = st_intersects spatial relations spatial subsetting operations base R).Bonus: create straight line centroid District Columbia near East coast centroid California near West coast USA (hint: functions st_centroid(), st_union() st_cast() described Chapter 5 may help) identify states long East-West line crosses.E5. Use dem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\")), reclassify elevation three classes: low (<300), medium high (>500).\nSecondly, read NDVI raster (ndvi = rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))) compute mean NDVI mean elevation altitudinal class.E6. Apply line detection filter rast(system.file(\"ex/logo.tif\", package = \"terra\")).\nPlot result.\nHint: Read ?terra::focal().E7. Calculate Normalized Difference Water Index (NDWI; (green - nir)/(green + nir)) Landsat image.\nUse Landsat image provided spDataLarge package (system.file(\"raster/landsat.tif\", package = \"spDataLarge\")).\nAlso, calculate correlation NDVI NDWI area (hint: can use layerCor() function).E8. StackOverflow post shows compute distances nearest coastline using raster::distance().\nTry something similar terra::distance(): retrieve digital elevation model Spain, compute raster represents distances coast across country (hint: use geodata::elevation_30s()).\nConvert resulting distances meters kilometers.\nNote: may wise increase cell size input raster reduce compute time operation (aggregate()).E9. Try modify approach used exercise weighting distance raster elevation raster; every 100 altitudinal meters increase distance coast 10 km.\nNext, compute visualize difference raster created using Euclidean distance (E7) raster weighted elevation.","code":"\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spData)\ncanterbury = nz |> filter(Name == \"Canterbury\")\ncanterbury_height = nz_height[canterbury, ]\nnz_not_canterbury_height = nz_height[canterbury, , op = st_disjoint]\nnrow(canterbury_height) # answer: 70\n#> [1] 70\n\nplot(st_geometry(nz))\nplot(st_geometry(canterbury), col = \"yellow\", add = TRUE)\nplot(nz_not_canterbury_height$geometry, pch = 1, col = \"blue\", add = TRUE)\nplot(canterbury_height$geometry, pch = 4, col = \"red\", add = TRUE)\nnz_height_count = aggregate(nz_height, nz, length)\nnz_height_combined = cbind(nz, count = nz_height_count$elevation)\nnz_height_combined |> \n  st_drop_geometry() |> \n  select(Name, count) |> \n  arrange(desc(count)) |> \n  slice(2)\n#>         Name count\n#> 1 West Coast    22\n# Base R way:\nnz_height_count = aggregate(nz_height, nz, length)\nnz_height_combined = cbind(nz, count = nz_height_count$elevation)\nplot(nz_height_combined)\n\n# Tidyverse way:\nnz_height_joined = st_join(nz_height, nz |> select(Name))\n# Calculate n. points in each region - this contains the result\nnz_height_counts = nz_height_joined |> \n  group_by(Name) |> \n  summarise(count = n())\n\n# Optionally join results with nz geometries:\nnz_height_combined = left_join(nz, nz_height_counts |> sf::st_drop_geometry())\n#> Joining with `by = join_by(Name)`\n# plot(nz_height_combined) # Check: results identical to base R result\n\n# Generate a summary table\nnz_height_combined |> \n  st_drop_geometry() |> \n  select(Name, count) |> \n  arrange(desc(count)) |> \n  na.omit()\n#>                Name count\n#> 1        Canterbury    70\n#> 2        West Coast    22\n#> 3           Waikato     3\n#> 4 Manawatu-Wanganui     2\n#> 5             Otago     2\n#> 6         Southland     1\n#> 7       Marlborough     1\ncolorado = us_states[us_states$NAME == \"Colorado\", ]\nplot(us_states$geometry)\nplot(colorado$geometry, col = \"grey\", add = TRUE)\nintersects_with_colorado = us_states[colorado, , op = st_intersects]\nplot(us_states$geometry, main = \"States that intersect with Colorado\")\nplot(intersects_with_colorado$geometry, col = \"grey\", add = TRUE)\n# Alternative but more verbose solutions\n# 2: With intermediate object, one list for each state\nsel_intersects_colorado = st_intersects(us_states, colorado)\nsel_intersects_colorado_list = lengths(sel_intersects_colorado) > 0\nintersects_with_colorado = us_states[sel_intersects_colorado_list, ]\n\n# 3: With intermediate object, one index for each state\nsel_intersects_colorado2 = st_intersects(colorado, us_states)\nsel_intersects_colorado2\n#> Sparse geometry binary predicate list of length 1, where the predicate\n#> was `intersects'\n#>  1: 2, 3, 9, 19, 37, 39, 45, 49\nus_states$NAME[unlist(sel_intersects_colorado2)]\n#> [1] \"Arizona\"    \"Colorado\"   \"Kansas\"     \"Oklahoma\"   \"Nebraska\"  \n#> [6] \"New Mexico\" \"Utah\"       \"Wyoming\"\n\n# 4: With tidyverse\nus_states |> \n  st_filter(y = colorado, .predicate = st_intersects)\n#> Simple feature collection with 8 features and 6 fields\n#> Geometry type: MULTIPOLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: -115 ymin: 31.3 xmax: -94.4 ymax: 45\n#> Geodetic CRS:  NAD83\n#>   GEOID       NAME  REGION          AREA total_pop_10 total_pop_15\n#> 1    04    Arizona    West 295281 [km^2]      6246816      6641928\n#> 2    08   Colorado    West 269573 [km^2]      4887061      5278906\n#> 3    20     Kansas Midwest 213037 [km^2]      2809329      2892987\n#> 4    40   Oklahoma   South 180971 [km^2]      3675339      3849733\n#> 5    31   Nebraska Midwest 200272 [km^2]      1799125      1869365\n#> 6    35 New Mexico    West 314886 [km^2]      2013122      2084117\n#> 7    49       Utah    West 219860 [km^2]      2657236      2903379\n#> 8    56    Wyoming    West 253310 [km^2]       545579       579679\n#>                         geometry\n#> 1 MULTIPOLYGON (((-115 32.7, ...\n#> 2 MULTIPOLYGON (((-109 41, -1...\n#> 3 MULTIPOLYGON (((-102 40, -1...\n#> 4 MULTIPOLYGON (((-103 37, -1...\n#> 5 MULTIPOLYGON (((-104 43, -1...\n#> 6 MULTIPOLYGON (((-109 37, -1...\n#> 7 MULTIPOLYGON (((-114 42, -1...\n#> 8 MULTIPOLYGON (((-104 45, -1...\ntouches_colorado = us_states[colorado, , op = st_touches]\nplot(us_states$geometry, main = \"States that touch Colorado\")\nplot(touches_colorado$geometry, col = \"grey\", add = TRUE)\nwashington_to_cali = us_states |> \n  filter(grepl(pattern = \"Columbia|Cali\", x = NAME)) |> \n  st_centroid() |> \n  st_union() |> \n  st_cast(\"LINESTRING\")\n#> Warning: st_centroid assumes attributes are constant over geometries\nstates_crossed = us_states[washington_to_cali, , op = st_crosses]\n#> although coordinates are longitude/latitude, st_crosses assumes that they are\n#> planar\nstates_crossed$NAME\n#>  [1] \"Colorado\"             \"Indiana\"              \"Kansas\"              \n#>  [4] \"Missouri\"             \"Nevada\"               \"West Virginia\"       \n#>  [7] \"California\"           \"District of Columbia\" \"Illinois\"            \n#> [10] \"Kentucky\"             \"Ohio\"                 \"Utah\"                \n#> [13] \"Virginia\"\nplot(us_states$geometry, main = \"States crossed by a straight line\\n from the District of Columbia to central California\")\nplot(states_crossed$geometry, col = \"grey\", add = TRUE)\nplot(washington_to_cali, add = TRUE)\nlibrary(terra)\n#> terra 1.7.55\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nndvi = rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))\n\n#1\ndem_rcl = matrix(c(-Inf, 300, 0, 300, 500, 1, 500, Inf, 2), ncol = 3, byrow = TRUE)\ndem_reclass = classify(dem, dem_rcl)\nlevels(dem_reclass) = data.frame(id = 0:2, cats = c(\"low\", \"medium\", \"high\"))\nplot(dem_reclass)\n\n#2\nzonal(c(dem, ndvi), dem_reclass, fun = \"mean\")\n#>     cats dem   ndvi\n#> 1    low 274 -0.363\n#> 2 medium 392 -0.289\n#> 3   high 765 -0.208\n# from the focal help page (?terra::focal()):\n# Laplacian filter: filter=matrix(c(0,1,0,1,-4,1,0,1,0), nrow=3)\n# Sobel filters (for edge detection): \n# fx=matrix(c(-1,-2,-1,0,0,0,1,2,1), nrow=3) \n# fy=matrix(c(1,0,-1,2,0,-2,1,0,-1), nrow=3)\n\n# just retrieve the first channel of the R logo\nr = rast(system.file(\"ex/logo.tif\", package = \"terra\"))\n# compute the Sobel filter\nfilter_x = matrix(c(-1, -2, -1, 0, 0, 0, 1, 2, 1), nrow = 3)\nsobel_x = focal(r, w = filter_x)\nplot(sobel_x, col = c(\"white\", \"black\"))\n\nfilter_y = matrix(c(1, 0, -1, 2, 0, -2, 1, 0, -1), nrow = 3)\nsobel_y = focal(r, w = filter_y)\nplot(sobel_y, col = c(\"black\", \"white\"))\nfile = system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nmulti_rast = rast(file)\n\nndvi_fun = function(nir, red){\n  (nir - red) / (nir + red)\n}\nndvi_rast = lapp(multi_rast[[c(4, 3)]], fun = ndvi_fun)\nplot(ndvi_rast)\n\nndwi_fun = function(green, nir){\n    (green - nir) / (green + nir)\n}\n\nndwi_rast = lapp(multi_rast[[c(2, 4)]], fun = ndwi_fun)\nplot(ndwi_rast)\n\ntwo_rasts = c(ndvi_rast, ndwi_rast)\nnames(two_rasts) = c(\"ndvi\", \"ndwi\")\n\n# correlation -- option 1\nlayerCor(two_rasts, fun = cor)\n#>        ndvi   ndwi\n#> ndvi  1.000 -0.913\n#> ndwi -0.913  1.000\n\n# correlation -- option 2\ntwo_rasts_df = as.data.frame(two_rasts)\ncor(two_rasts_df$ndvi, two_rasts_df$ndwi)\n#> [1] -0.913# Fetch the DEM data for Spain\nspain_dem = geodata::elevation_30s(country = \"Spain\", path = \".\", mask = FALSE)\n\n# Reduce the resolution by a factor of 20 to speed up calculations\nspain_dem = aggregate(spain_dem, fact = 20)\n\n# According to the documentation, terra::distance() will calculate distance\n# for all cells that are NA to the nearest cell that are not NA. To calculate\n# distance to the coast, we need a raster that has NA values over land and any\n# other value over water\nwater_mask = is.na(spain_dem)\nwater_mask[water_mask == 0] = NA\n\n# Use the distance() function on this mask to get distance to the coast\ndistance_to_coast = distance(water_mask)\n#> \n|---------|---------|---------|---------|\n=========================================\n                                          \n# convert distance into km\ndistance_to_coast_km = distance_to_coast / 1000\n\n# Plot the result\nplot(distance_to_coast_km, main = \"Distance to the coast (km)\")\n# now let's weight each 100 altitudinal meters by an additional distance of 10 km\ndistance_to_coast_km2 = distance_to_coast_km + ((spain_dem / 100) * 10)\n# plot the result\nplot(distance_to_coast_km2)\n# visualize the difference\nplot(distance_to_coast_km - distance_to_coast_km2)"},{"path":"geometric-operations.html","id":"geometric-operations","chapter":"5 Geometry operations","heading":"5 Geometry operations","text":"E1. Generate plot simplified versions nz dataset.\nExperiment different values keep (ranging 0.5 0.00005) ms_simplify() dTolerance (100 100,000) st_simplify().value form result start break method, making New Zealand unrecognizable?Advanced: different geometry type results st_simplify() compared geometry type ms_simplify()? problems create can resolved?E2. first exercise Chapter Spatial data operations established Canterbury region 70 101 highest points New Zealand.\nUsing st_buffer(), many points nz_height within 100 km Canterbury?E3. Find geographic centroid New Zealand.\nfar geographic centroid Canterbury?E4. world maps north-orientation.\nworld map south-orientation created reflection (one affine transformations mentioned chapter) world object’s geometry.\nWrite code .\nHint: need use two-element vector transformation.\nBonus: create upside-map country.E5. Run code Section 5.2.6. reference objects created section, subset point p contained within x y.Using base subsetting operators.Using intermediary object created st_intersection().E6. Calculate length boundary lines US states meters.\nstate longest border shortest?\nHint: st_length function computes length LINESTRING MULTILINESTRING geometry.E7. Read srtm.tif file R (srtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))).\nraster resolution 0.00083 0.00083 degrees.\nChange resolution 0.01 0.01 degrees using method available terra package.\nVisualize results.\nCan notice differences results resampling methods?","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spDataLarge)\nplot(rmapshaper::ms_simplify(st_geometry(nz), keep = 0.5))\nplot(rmapshaper::ms_simplify(st_geometry(nz), keep = 0.05))\n# Starts to breakdown here at 0.5% of the points:\nplot(rmapshaper::ms_simplify(st_geometry(nz), keep = 0.005))\n# At this point no further simplification changes the result\nplot(rmapshaper::ms_simplify(st_geometry(nz), keep = 0.0005))\nplot(rmapshaper::ms_simplify(st_geometry(nz), keep = 0.00005))\nplot(st_simplify(st_geometry(nz), dTolerance = 100))\nplot(st_simplify(st_geometry(nz), dTolerance = 1000))\n# Starts to breakdown at 10 km:\nplot(st_simplify(st_geometry(nz), dTolerance = 10000))\nplot(st_simplify(st_geometry(nz), dTolerance = 100000))\nplot(st_simplify(st_geometry(nz), dTolerance = 100000, preserveTopology = TRUE))\n\n# Problem: st_simplify returns POLYGON and MULTIPOLYGON results, affecting plotting\n# Cast into a single geometry type to resolve this\nnz_simple_poly = st_simplify(st_geometry(nz), dTolerance = 10000) |> \n  st_sfc() |> \n  st_cast(\"POLYGON\")\n#> Warning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\n\n#> Warning in st_cast.MULTIPOLYGON(X[[i]], ...): polygon from first part only\nnz_simple_multipoly = st_simplify(st_geometry(nz), dTolerance = 10000) |> \n  st_sfc() |> \n  st_cast(\"MULTIPOLYGON\")\nplot(nz_simple_poly)\nlength(nz_simple_poly)\n#> [1] 16\nnrow(nz)\n#> [1] 16\ncanterbury = nz[nz$Name == \"Canterbury\", ]\ncant_buff = st_buffer(canterbury, 100)\nnz_height_near_cant = nz_height[cant_buff, ]\nnrow(nz_height_near_cant) # 75 - 5 more\n#> [1] 75\ncant_cent = st_centroid(canterbury)\n#> Warning: st_centroid assumes attributes are constant over geometries\nnz_centre = st_centroid(st_union(nz))\nst_distance(cant_cent, nz_centre) # 234 km\n#> Units: [m]\n#>        [,1]\n#> [1,] 234193\nworld_sfc = st_geometry(world)\nworld_sfc_mirror = world_sfc * c(1, -1)\n#> Warning in mapply(function(x, y) {: longer argument not a multiple of length of\n#> shorter\nplot(world_sfc)\nplot(world_sfc_mirror)\n\nus_states_sfc = st_geometry(us_states)\nus_states_sfc_mirror = us_states_sfc * c(1, -1)\n#> Warning in mapply(function(x, y) {: longer argument not a multiple of length of\n#> shorter\nplot(us_states_sfc)\nplot(us_states_sfc_mirror)\n## nicer plot\n# library(ggrepel)\n# us_states_sfc_mirror_labels = st_centroid(us_states_sfc_mirror) |> \n#   st_coordinates() |>\n#   as_data_frame() |> \n#   mutate(name = us_states$NAME)\n# us_states_sfc_mirror_sf = st_set_geometry(us_states, us_states_sfc_mirror)\n# ggplot(data = us_states_sfc_mirror_sf) +\n#   geom_sf(color = \"white\") +\n#   geom_text_repel(data = us_states_sfc_mirror_labels, mapping = aes(X, Y, label = name), size = 3, min.segment.length = 0) +\n#   theme_void() \np_in_y = p[y]\np_in_xy = p_in_y[x]\nx_and_y = st_intersection(x, y)\np[x_and_y]\n#> Geometry set for 1 feature \n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 0.305 ymin: 1.43 xmax: 0.305 ymax: 1.43\n#> CRS:           NA\n#> POINT (0.305 1.43)\nus_states2163 = st_transform(us_states, \"EPSG:2163\")\nus_states_bor = st_cast(us_states2163, \"MULTILINESTRING\")\nus_states_bor$borders = st_length(us_states_bor)\narrange(us_states_bor, borders)\n#> Simple feature collection with 49 features and 7 fields\n#> Geometry type: MULTILINESTRING\n#> Dimension:     XY\n#> Bounding box:  xmin: -2040000 ymin: -2110000 xmax: 2520000 ymax: 731000\n#> Projected CRS: NAD27 / US National Atlas Equal Area\n#> First 10 features:\n#>    GEOID                 NAME   REGION         AREA total_pop_10 total_pop_15\n#> 1     11 District of Columbia    South   178 [km^2]       584400       647484\n#> 2     44         Rhode Island Norteast  2743 [km^2]      1056389      1053661\n#> 3     10             Delaware    South  5182 [km^2]       881278       926454\n#> 4     09          Connecticut Norteast 12977 [km^2]      3545837      3593222\n#> 5     34           New Jersey Norteast 20274 [km^2]      8721577      8904413\n#> 6     50              Vermont Norteast 24866 [km^2]       624258       626604\n#> 7     33        New Hampshire Norteast 24026 [km^2]      1313939      1324201\n#> 8     25        Massachusetts Norteast 20911 [km^2]      6477096      6705586\n#> 9     45       South Carolina    South 80904 [km^2]      4511428      4777576\n#> 10    18              Indiana  Midwest 93648 [km^2]      6417398      6568645\n#>        borders                       geometry\n#> 1    60330 [m] MULTILINESTRING ((1955479 -...\n#> 2   304697 [m] MULTILINESTRING ((2338362 4...\n#> 3   407939 [m] MULTILINESTRING ((2041302 -...\n#> 4   514571 [m] MULTILINESTRING ((2148010 2...\n#> 5   747090 [m] MULTILINESTRING ((2062773 -...\n#> 6   778660 [m] MULTILINESTRING ((2054045 3...\n#> 7   782892 [m] MULTILINESTRING ((2188613 3...\n#> 8  1018656 [m] MULTILINESTRING ((2422968 3...\n#> 9  1275538 [m] MULTILINESTRING ((1534988 -...\n#> 10 1436255 [m] MULTILINESTRING ((1033797 -...\narrange(us_states_bor, -borders)\n#> Simple feature collection with 49 features and 7 fields\n#> Geometry type: MULTILINESTRING\n#> Dimension:     XY\n#> Bounding box:  xmin: -2040000 ymin: -2110000 xmax: 2520000 ymax: 731000\n#> Projected CRS: NAD27 / US National Atlas Equal Area\n#> First 10 features:\n#>    GEOID       NAME  REGION          AREA total_pop_10 total_pop_15     borders\n#> 1     48      Texas   South 687714 [km^2]     24311891     26538614 4959186 [m]\n#> 2     06 California    West 409747 [km^2]     36637290     38421464 3810350 [m]\n#> 3     26   Michigan Midwest 151119 [km^2]      9952687      9900571 3579183 [m]\n#> 4     12    Florida   South 151052 [km^2]     18511620     19645772 2949048 [m]\n#> 5     30    Montana    West 380829 [km^2]       973739      1014699 2826586 [m]\n#> 6     16      Idaho    West 216513 [km^2]      1526797      1616547 2570606 [m]\n#> 7     27  Minnesota Midwest 218566 [km^2]      5241914      5419171 2567389 [m]\n#> 8     51   Virginia   South 105405 [km^2]      7841754      8256630 2407299 [m]\n#> 9     35 New Mexico    West 314886 [km^2]      2013122      2084117 2378172 [m]\n#> 10    53 Washington    West 175436 [km^2]      6561297      6985464 2344358 [m]\n#>                          geometry\n#> 1  MULTILINESTRING ((-269579 -...\n#> 2  MULTILINESTRING ((-1720565 ...\n#> 3  MULTILINESTRING ((1113866 1...\n#> 4  MULTILINESTRING ((1855611 -...\n#> 5  MULTILINESTRING ((-1165102 ...\n#> 6  MULTILINESTRING ((-1298509 ...\n#> 7  MULTILINESTRING ((202873 44...\n#> 8  MULTILINESTRING ((2104821 -...\n#> 9  MULTILINESTRING ((-805019 -...\n#> 10 MULTILINESTRING ((-1663690 ...\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nrast_template = rast(ext(srtm), res = 0.01)\nsrtm_resampl1 = resample(srtm, y = rast_template, method = \"bilinear\")\nsrtm_resampl2 = resample(srtm, y = rast_template, method = \"near\")\nsrtm_resampl3 = resample(srtm, y = rast_template, method = \"cubic\")\nsrtm_resampl4 = resample(srtm, y = rast_template, method = \"cubicspline\")\nsrtm_resampl5 = resample(srtm, y = rast_template, method = \"lanczos\")\n\nsrtm_resampl_all = c(srtm_resampl1, srtm_resampl2, srtm_resampl3,\n                     srtm_resampl4, srtm_resampl5)\nplot(srtm_resampl_all)\n\n# differences\nplot(srtm_resampl_all - srtm_resampl1, range = c(-300, 300))\nplot(srtm_resampl_all - srtm_resampl2, range = c(-300, 300))\nplot(srtm_resampl_all - srtm_resampl3, range = c(-300, 300))\nplot(srtm_resampl_all - srtm_resampl4, range = c(-300, 300))\nplot(srtm_resampl_all - srtm_resampl5, range = c(-300, 300))"},{"path":"raster-vector.html","id":"raster-vector","chapter":"6 Raster-vector interactions","heading":"6 Raster-vector interactions","text":"following exercises use vector (zion_points) raster dataset (srtm) spDataLarge package.\nalso use polygonal ‘convex hull’ derived vector dataset (ch) represent area interest:E1. Crop srtm raster using (1) zion_points dataset (2) ch dataset.\ndifferences output maps?\nNext, mask srtm using two datasets.\nCan see difference now?\ncan explain ?E2. Firstly, extract values srtm points represented zion_points.\nNext, extract average values srtm using 90 buffer around point zion_points compare two sets values.\nextracting values buffers suitable points alone?Bonus: Implement extraction using exactextractr package compare results.E3. Subset points higher 3100 meters New Zealand (nz_height object) create template raster resolution 3 km extent new point dataset.\nUsing two new objects:Count numbers highest points grid cell.Find maximum elevation grid cell.E4. Aggregate raster counting high points New Zealand (created previous exercise), reduce geographic resolution half (cells 6 6 km) plot result.Resample lower resolution raster back original resolution 3 km. results changed?Name two advantages disadvantages reducing raster resolution.Advantages:lower memory usefaster processinggood viz casesDisadvantages:removes geographic detailadds another processing stepE5. Polygonize grain dataset filter squares representing clay.Name two advantages disadvantages vector data raster data.useful convert rasters vectors work?Advantages:can used subset vector objectscan affine transformations use sf/dplyr verbsDisadvantages:better consistencyfast processing operationsfunctions developed domains","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nzion_points_path = system.file(\"vector/zion_points.gpkg\", package = \"spDataLarge\")\nzion_points = read_sf(zion_points_path)\nsrtm = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\nch = st_combine(zion_points) |>\n  st_convex_hull() |> \n  st_as_sf()\nplot(srtm)\nplot(st_geometry(zion_points), add = TRUE)\nplot(ch, add = TRUE)\n\nsrtm_crop1 = crop(srtm, zion_points)\nsrtm_crop2 = crop(srtm, ch)\nplot(srtm_crop1)\nplot(srtm_crop2)\n\nsrtm_mask1 = mask(srtm, zion_points)\nsrtm_mask2 = mask(srtm, ch)\nplot(srtm_mask1)\nplot(srtm_mask2)\nzion_points_buf = st_buffer(zion_points, dist = 90)\nplot(srtm)\nplot(st_geometry(zion_points_buf), add = TRUE)\nplot(ch, add = TRUE)\n\nzion_points_points = extract(srtm, zion_points)\nzion_points_buffer = extract(srtm, zion_points_buf, fun = \"mean\")\nplot(zion_points_points$srtm, zion_points_buffer$srtm)\n\n# Bonus\n# remotes::install_github(\"isciences/exactextractr\")\n# zion_points_buf_2 = exactextractr::exact_extract(x = srtm, y = zion_points_buf,\n#                                                  fun = \"mean\")\n# \n# plot(zion_points_points$srtm, zion_points_buf_2)\n# plot(zion_points_buffer$srtm, zion_points_buf_2)\nnz_height3100 = dplyr::filter(nz_height, elevation > 3100)\nnew_graticule = st_graticule(nz_height3100, datum = \"EPSG:2193\")\nplot(st_geometry(nz_height3100), graticule = new_graticule, axes = TRUE)\n\nnz_template = rast(ext(nz_height3100), resolution = 3000, crs = crs(nz_height3100))\n\nnz_raster = rasterize(nz_height3100, nz_template, \n                      field = \"elevation\", fun = \"length\")\nplot(nz_raster)\nplot(st_geometry(nz_height3100), add = TRUE)\n\nnz_raster2 = rasterize(nz_height3100, nz_template, \n                       field = \"elevation\", fun = max)\nplot(nz_raster2)\nplot(st_geometry(nz_height3100), add = TRUE)\nnz_raster_low = raster::aggregate(nz_raster, fact = 2, fun = sum, na.rm = TRUE)\nres(nz_raster_low)\n#> [1] 6000 6000\n\nnz_resample = resample(nz_raster_low, nz_raster)\nplot(nz_raster_low)\nplot(nz_resample) # the results are spread over a greater area and there are border issues\nplot(nz_raster)\ngrain = rast(system.file(\"raster/grain.tif\", package = \"spData\"))\ngrain_poly = as.polygons(grain) |> \n  st_as_sf()\nlevels(grain)\n#> [[1]]\n#>   value grain\n#> 1     0  clay\n#> 2     1  silt\n#> 3     2  sand\nclay = dplyr::filter(grain_poly, grain == \"clay\")\nplot(clay)"},{"path":"reproj-geo-data.html","id":"reproj-geo-data","chapter":"7 Reprojecting geographic data","heading":"7 Reprojecting geographic data","text":"E1. Create new object called nz_wgs transforming nz object WGS84 CRS.Create object class crs use query CRSs.reference bounding box object, units CRS use?Remove CRS nz_wgs plot result: wrong map New Zealand ?E2. Transform world dataset transverse Mercator projection (\"+proj=tmerc\") plot result.\nchanged ?\nTry transform back WGS 84 plot new object.\nnew object differ original one?E3. Transform continuous raster (con_raster) NAD83 / UTM zone 12N using nearest neighbor interpolation method.\nchanged?\ninfluence results?E4. Transform categorical raster (cat_raster) WGS 84 using bilinear interpolation method.\nchanged?\ninfluence results?","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(spData)\nst_crs(nz)\n#> Coordinate Reference System:\n#>   User input: EPSG:2193 \n#>   wkt:\n#> PROJCS[\"NZGD2000 / New Zealand Transverse Mercator 2000\",\n#>     GEOGCS[\"NZGD2000\",\n#>         DATUM[\"New_Zealand_Geodetic_Datum_2000\",\n#>             SPHEROID[\"GRS 1980\",6378137,298.257222101,\n#>                 AUTHORITY[\"EPSG\",\"7019\"]],\n#>             TOWGS84[0,0,0,0,0,0,0],\n#>             AUTHORITY[\"EPSG\",\"6167\"]],\n#>         PRIMEM[\"Greenwich\",0,\n#>             AUTHORITY[\"EPSG\",\"8901\"]],\n#>         UNIT[\"degree\",0.0174532925199433,\n#>             AUTHORITY[\"EPSG\",\"9122\"]],\n#>         AUTHORITY[\"EPSG\",\"4167\"]],\n#>     PROJECTION[\"Transverse_Mercator\"],\n#>     PARAMETER[\"latitude_of_origin\",0],\n#>     PARAMETER[\"central_meridian\",173],\n#>     PARAMETER[\"scale_factor\",0.9996],\n#>     PARAMETER[\"false_easting\",1600000],\n#>     PARAMETER[\"false_northing\",10000000],\n#>     UNIT[\"metre\",1,\n#>         AUTHORITY[\"EPSG\",\"9001\"]],\n#>     AUTHORITY[\"EPSG\",\"2193\"]]\nnz_wgs = st_transform(nz, \"EPSG:4326\")\nnz_crs = st_crs(nz)\nnz_wgs_crs = st_crs(nz_wgs)\nnz_crs$epsg\n#> [1] 2193\nnz_wgs_crs$epsg\n#> [1] 4326\nst_bbox(nz)\n#>    xmin    ymin    xmax    ymax \n#> 1090144 4748537 2089533 6191874\nst_bbox(nz_wgs)\n#>  xmin  ymin  xmax  ymax \n#> 166.4 -47.3 178.6 -34.4\nnz_wgs_NULL_crs = st_set_crs(nz_wgs, NA)\nnz_27700 = st_transform(nz_wgs, \"EPSG:27700\")\npar(mfrow = c(1, 3))\nplot(st_geometry(nz))\nplot(st_geometry(nz_wgs))\nplot(st_geometry(nz_wgs_NULL_crs))\n# answer: it is fatter in the East-West direction\n# because New Zealand is close to the South Pole and meridians converge there\nplot(st_geometry(nz_27700))\npar(mfrow = c(1, 1))\n# see https://github.com/r-spatial/sf/issues/509\nworld_tmerc = st_transform(world, \"+proj=tmerc\")\nplot(st_geometry(world_tmerc))\nworld_4326 = st_transform(world_tmerc, \"EPSG:4326\")\nplot(st_geometry(world_4326))\ncon_raster = rast(system.file(\"raster/srtm.tif\", package = \"spDataLarge\"))\ncon_raster_utm12n = project(con_raster, \"EPSG:32612\", method = \"near\")\ncon_raster_utm12n\n#> class       : SpatRaster \n#> dimensions  : 515, 422, 1  (nrow, ncol, nlyr)\n#> resolution  : 83.5, 83.5  (x, y)\n#> extent      : 301062, 336313, 4111111, 4154131  (xmin, xmax, ymin, ymax)\n#> coord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \n#> source(s)   : memory\n#> name        : srtm \n#> min value   : 1024 \n#> max value   : 2892\n\nplot(con_raster)\nplot(con_raster_utm12n)\ncat_raster = rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\ncat_raster_wgs84 = project(cat_raster, \"EPSG:4326\", method = \"bilinear\")\ncat_raster_wgs84\n#> class       : SpatRaster \n#> dimensions  : 1246, 1244, 1  (nrow, ncol, nlyr)\n#> resolution  : 0.000315, 0.000315  (x, y)\n#> extent      : -113, -113, 37.1, 37.5  (xmin, xmax, ymin, ymax)\n#> coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#> source(s)   : memory\n#> name        : levels \n#> min value   :      1 \n#> max value   :      8\n\nplot(cat_raster)\nplot(cat_raster_wgs84)"},{"path":"read-write.html","id":"read-write","chapter":"8 Geographic data I/O","heading":"8 Geographic data I/O","text":"E1. List describe three types vector, raster, geodatabase formats.Vector formats: Shapefile (old format supported many programs), GeoPackage (recent format better support attribute data) GeoJSON (common format web mapping).Raster formats: GeoTiff, Arc ASCII, ERDAS Imagine (IMG).Database formats: PostGIS, SQLite, FileGDB.E2. Name least two differences sf functions read_sf() st_read().st_read() prints outputs keeps strings text strings (st_read() creates factors). can seen source code read_sf(), show’s wraps st_read():E3. Read cycle_hire_xy.csv file spData package spatial object (Hint: located misc folder).\ngeometry type loaded object?E4. Download borders Germany using rnaturalearth, create new object called germany_borders.\nWrite new object file GeoPackage format.E5. Download global monthly minimum temperature spatial resolution five minutes using geodata package.\nExtract June values, save file named tmin_june.tif file (hint: use terra::subset()).E6. Create static map Germany’s borders, save PNG file.E7. Create interactive map using data cycle_hire_xy.csv file.\nExport map file called cycle_hire.html.","code":"\nlibrary(sf)\nlibrary(terra)\nread_sf\n#> function (..., quiet = TRUE, stringsAsFactors = FALSE, as_tibble = TRUE) \n#> {\n#>     st_read(..., quiet = quiet, stringsAsFactors = stringsAsFactors, \n#>         as_tibble = as_tibble)\n#> }\n#> <bytecode: 0x55801a1aa4d8>\n#> <environment: namespace:sf>\nc_h = read.csv(system.file(\"misc/cycle_hire_xy.csv\", package = \"spData\")) |> \n  st_as_sf(coords = c(\"X\", \"Y\"))\nc_h\n#> Simple feature collection with 742 features and 5 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -0.237 ymin: 51.5 xmax: -0.00228 ymax: 51.5\n#> CRS:           NA\n#> First 10 features:\n#>    id               name             area nbikes nempty             geometry\n#> 1   1       River Street      Clerkenwell      4     14   POINT (-0.11 51.5)\n#> 2   2 Phillimore Gardens       Kensington      2     34  POINT (-0.198 51.5)\n#> 3   3 Christopher Street Liverpool Street      0     32 POINT (-0.0846 51.5)\n#> 4   4  St. Chad's Street     King's Cross      4     19  POINT (-0.121 51.5)\n#> 5   5     Sedding Street    Sloane Square     15     12  POINT (-0.157 51.5)\n#> 6   6 Broadcasting House       Marylebone      0     18  POINT (-0.144 51.5)\n#> 7   7   Charlbert Street  St. John's Wood     15      0  POINT (-0.168 51.5)\n#> 8   8         Lodge Road  St. John's Wood      5     13   POINT (-0.17 51.5)\n#> 9   9     New Globe Walk         Bankside      3     16 POINT (-0.0964 51.5)\n#> 10 10        Park Street         Bankside      1     17 POINT (-0.0928 51.5)\nlibrary(rnaturalearth)\n#> Support for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\ngermany_borders = ne_countries(country = \"Germany\", returnclass = \"sf\")\nplot(germany_borders)\n#> Warning: plotting the first 10 out of 168 attributes; use max.plot = 168 to\n#> plot all\nst_write(germany_borders, \"germany_borders.gpkg\")\n#> Writing layer `germany_borders' to data source \n#>   `germany_borders.gpkg' using driver `GPKG'\n#> Writing 1 features with 168 fields and geometry type Polygon.\nlibrary(geodata)\ngmmt = worldclim_global(var = \"tmin\", res = 5, path = tempdir())\nnames(gmmt)\n#>  [1] \"wc2.1_5m_tmin_01\" \"wc2.1_5m_tmin_02\" \"wc2.1_5m_tmin_03\" \"wc2.1_5m_tmin_04\"\n#>  [5] \"wc2.1_5m_tmin_05\" \"wc2.1_5m_tmin_06\" \"wc2.1_5m_tmin_07\" \"wc2.1_5m_tmin_08\"\n#>  [9] \"wc2.1_5m_tmin_09\" \"wc2.1_5m_tmin_10\" \"wc2.1_5m_tmin_11\" \"wc2.1_5m_tmin_12\"\nplot(gmmt)\n\ngmmt_june = terra::subset(gmmt, \"wc2.1_5m_tmin_06\")\nplot(gmmt_june)\nwriteRaster(gmmt_june, \"tmin_june.tif\")\npng(filename = \"germany.png\", width = 350, height = 500)\nplot(st_geometry(germany_borders), axes = TRUE, graticule = TRUE)\ndev.off()\n#> png \n#>   2\nlibrary(mapview)\nmapview_obj = mapview(c_h, zcol = \"nbikes\", legend = TRUE)\nmapshot(mapview_obj, file = \"cycle_hire.html\")"},{"path":"adv-map.html","id":"adv-map","chapter":"9 Making maps with R","heading":"9 Making maps with R","text":"exercises rely new object, africa.\nCreate using world worldbank_df datasets spData package follows:also use zion nlcd datasets spDataLarge:E1. Create map showing geographic distribution Human Development Index (HDI) across Africa base graphics (hint: use plot()) tmap packages (hint: use tm_shape(africa) + ...).\n- Name two advantages based experience.\n- Name three mapping packages advantage .\n- Bonus: create three maps Africa using three packages.E2. Extend tmap created previous exercise legend three bins: “High” (HDI 0.7), “Medium” (HDI 0.55 0.7) “Low” (HDI 0.55).\n- Bonus: improve map aesthetics, example changing legend title, class labels color palette.E3. Represent africa’s subregions map.\nChange default color palette legend title.\nNext, combine map map created previous exercise single plot.E4. Create land cover map Zion National Park.\n- Change default colors match perception land cover categories\n- Add scale bar north arrow change position improve map’s aesthetic appeal\n- Bonus: Add inset map Zion National Park’s location context Utah state. (Hint: object representing Utah can subset us_states dataset.)E5. Create facet maps countries Eastern Africa:\n- one facet showing HDI representing population growth (hint: using variables HDI pop_growth, respectively)\n- ‘small multiple’ per countryE6. Building previous facet map examples, create animated maps East Africa:\n- Showing country order\n- Showing country order legend showing HDIE7. Create interactive map HDI Africa:\n- tmap\n- mapview\n- leaflet\n- Bonus: approach, add legend (automatically provided) scale barE8. Sketch paper ideas web mapping app used make transport land-use policies evidence based:\n- city live, couple users per day\n- country live, dozens users per day\n- Worldwide hundreds users per day large data serving requirementsIdeas include identification routes many people currently drive short distances, ways encourage access parks, prioritization new developments reduce long-distance travel.city level web map sufficient.national level mapping application, e.g. shiny, probably needed.Worldwide, database serve data likely needed. various front-ends plug .E9. Update code coffeeApp/app.R instead centering Brazil user can select country focus :\n- Using textInput()\n- Using selectInput()answer can found shinymod branch geocompr repo: https://github.com/Robinlovelace/geocompr/pull/318/files\ncreate new widget use set center.\nNote: input data must fed map earlier prevent polygons disappearing change center way.E10. Reproduce Figure 9.1 Figure 9.7 closely possible using ggplot2 package.E11. Join us_states us_states_df together calculate poverty rate state using new dataset.\nNext, construct continuous area cartogram based total population.\nFinally, create compare two maps poverty rate: (1) standard choropleth map (2) map using created cartogram boundaries.\ninformation provided first second map?\ndiffer ?E12. Visualize population growth Africa.\nNext, compare maps hexagonal regular grid created using geogrid package.","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(spData)\nafrica = world |> \n  filter(continent == \"Africa\", !is.na(iso_a2)) |> \n  left_join(worldbank_df, by = \"iso_a2\") |> \n  select(name, subregion, gdpPercap, HDI, pop_growth) |> \n  st_transform(\"ESRI:102022\") |> \n  st_make_valid() |> \n  st_collection_extract(\"POLYGON\")\nzion = read_sf((system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")))\nnlcd = rast(system.file(\"raster/nlcd.tif\", package = \"spDataLarge\"))\n# graphics\nplot(africa[\"HDI\"])\n# tmap\nremotes::install_github(\"r-tmap/tmap\")\n#> Using github PAT from envvar GITHUB_PAT\n#> Skipping install of 'tmap' from a github remote, the SHA1 (b05b6dd3) has not changed since last install.\n#>   Use `force = TRUE` to force installation\nlibrary(tmap)\n#> \n#> Attaching package: 'tmap'\n#> The following object is masked from 'package:datasets':\n#> \n#>     rivers\ntm_shape(africa) + \n  tm_polygons(\"HDI\")\n# ggplot\nlibrary(ggplot2)\nggplot() +\n  geom_sf(data = africa, aes(fill = HDI))\n# ggplotly\nlibrary(plotly)\n#> \n#> Attaching package: 'plotly'\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     last_plot\n#> The following object is masked from 'package:stats':\n#> \n#>     filter\n#> The following object is masked from 'package:graphics':\n#> \n#>     layout\ng = ggplot() +\n  geom_sf(data = africa, aes(fill = HDI))\nggplotly(g)\nlibrary(tmap)\ntm_shape(africa) + \n  tm_polygons(\"HDI\",\n              fill.scale = tm_scale_intervals(breaks = c(0, 0.55, 0.7, 1),\n                                              labels = c(\"Low\", \"Medium\", \"High\"),\n                                              values = \"-viridis\"),\n              fill.legend = tm_legend(title = \"Human Development Index\")) \nasubregions = tm_shape(africa) +\n  tm_polygons(\"subregion\",\n              fill.scale = tm_scale_categorical(values = \"Set3\"),\n              fill.legend = tm_legend(title = \"Subregion:\"))\nahdi = tm_shape(africa) + \n  tm_polygons(\"HDI\",\n              fill.scale = tm_scale_intervals(breaks = c(0, 0.55, 0.7, 1),\n                                              labels = c(\"Low\", \"Medium\", \"High\"),\n                                              values = \"-viridis\"),\n              fill.legend = tm_legend(title = \"Human Development Index:\")) \ntmap_arrange(ahdi, asubregions)\n#> Multiple palettes called \"set3 found: \"brewer.set3\", \"hcl.set3\". The first one, \"brewer.set3\", is returned.\ntm_shape(nlcd) +\n  tm_raster(col.scale = tm_scale_categorical(values = c(\"#495EA1\", \"#AF5F63\", \"#EDE9E4\",\n                                                        \"#487F3F\", \"#EECFA8\", \"#A4D378\",\n                                                        \"#FFDB5C\", \"#72D593\"), levels.drop = TRUE)) +\n  tm_scalebar(bg.color = \"white\", position = c(\"left\", \"bottom\")) +\n  tm_compass(bg.color = \"white\", position = c(\"right\", \"top\")) +\n  tm_layout(legend.position = c(\"left\", \"top\"), legend.bg.color = \"white\")\n#> SpatRaster object downsampled to 1126 by 889 cells.\n# Bonus\nutah = subset(us_states, NAME == \"Utah\")\nutah = st_transform(utah, st_crs(zion))\n\nzion_region = st_bbox(zion) |> \n  st_as_sfc()\n\nmain = tm_shape(nlcd) +\n  tm_raster(col.scale = tm_scale_categorical(values = c(\"#495EA1\", \"#AF5F63\", \"#EDE9E4\",\n                                                        \"#487F3F\", \"#EECFA8\", \"#A4D378\",\n                                                        \"#FFDB5C\", \"#72D593\"), levels.drop = TRUE)) +\n  tm_scalebar(bg.color = \"white\", position = c(\"left\", \"bottom\")) +\n  tm_compass(bg.color = \"white\", position = c(\"right\", \"top\")) +\n  tm_layout(legend.position = c(\"left\", \"top\"), legend.bg.color = \"white\")\n\ninset = tm_shape(utah) +\n  tm_polygons() +\n  tm_text(\"UTAH\", size = 3) +\n  #tm_shape(zion) +\n  #tm_polygons(col = \"red\") +\n  tm_shape(zion_region) +\n  tm_borders(col = \"red\") +\n  tm_layout(frame = FALSE)\n\nlibrary(grid)\n#> \n#> Attaching package: 'grid'\n#> The following object is masked from 'package:terra':\n#> \n#>     depth\nnorm_dim = function(obj){\n    bbox = st_bbox(obj)\n    width = bbox[[\"xmax\"]] - bbox[[\"xmin\"]]\n    height = bbox[[\"ymax\"]] - bbox[[\"ymin\"]]\n    w = width / max(width, height)\n    h = height / max(width, height)\n    return(unit(c(w, h), \"snpc\"))\n}\nmain_dim = norm_dim(zion)\nins_dim = norm_dim(utah)\n\nmain_vp = viewport(width = main_dim[1], height = main_dim[2])\nins_vp = viewport(width = ins_dim[1] * 0.4, height = ins_dim[2] * 0.4,\n                  x = unit(1, \"npc\") - unit(0.5, \"cm\"), y = unit(0.5, \"cm\"),\n                  just = c(\"right\", \"bottom\"))\n\ngrid.newpage()\nprint(main, vp = main_vp)\n#> SpatRaster object downsampled to 1126 by 889 cells.\npushViewport(main_vp)\nprint(inset, vp = ins_vp)\nea = subset(africa, subregion == \"Eastern Africa\")\n#1\ntm_shape(ea) +\n  tm_polygons(c(\"HDI\", \"pop_growth\"))\n#2\ntm_shape(ea) +\n  tm_polygons() +\n  tm_facets_wrap(\"name\")\ntma1 = tm_shape(ea) +\n  tm_polygons() +\n  tm_facets(by = \"name\", nrow = 1, ncol = 1)\ntmap_animation(tma1, filename = \"tma2.gif\", width = 1000, height = 1000)\nbrowseURL(\"tma1.gif\")\n\ntma2 = tm_shape(africa) +\n  tm_polygons(fill = \"lightgrey\") +\n  tm_shape(ea) +\n  tm_polygons(fill = \"darkgrey\") +\n  tm_shape(ea) +\n  tm_polygons(fill = \"HDI\") +\n  tm_facets(by = \"name\", nrow = 1, ncol = 1)\ntmap_animation(tma2, filename = \"tma2.gif\", width = 1000, height = 1000)\nbrowseURL(\"tma2.gif\")\n# tmap\ntmap_mode(\"view\")\ntm_shape(africa) + tm_polygons(\"HDI\") + tm_scalebar()\n# mapview\nmapview::mapview(africa[\"HDI\"])\n# leaflet\nafrica4326 = st_transform(africa, \"EPSG:4326\")\nlibrary(leaflet)\npal = colorNumeric(palette = \"YlGnBu\", domain = africa4326$HDI)\nleaflet(africa4326) |> \n  addTiles() |> \n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1, color = ~pal(HDI)) |> \n  addLegend(\"bottomright\", pal = pal, values = ~HDI, opacity = 1) |> \n  addScaleBar()\nlibrary(ggplot2)\nggplot() + \n  geom_sf(data = nz, color = NA) +\n  coord_sf(crs = st_crs(nz), datum = NA) +\n  theme_void()\nggplot() +\n  geom_sf(data = nz, fill = NA) +\n  coord_sf(crs = st_crs(nz), datum = NA) +\n  theme_void()\nggplot() +\n  geom_sf(data = nz) + \n  coord_sf(crs = st_crs(nz), datum = NA) +\n  theme_void()\n# fig 9.7\nggplot() +\n  geom_sf(data = nz, aes(fill = Median_income)) +\n  coord_sf(crs = st_crs(nz), datum = NA) +\n  scale_fill_distiller(palette = \"Blues\", direction = 1) + \n  theme_void()\nggplot() + \n  geom_sf(data = nz, aes(fill = Island)) + \n  coord_sf(crs = st_crs(nz), datum = NA) +\n  scale_fill_manual(values = c(\"#CC6677\", \"#332288\")) + \n  theme_void()\ntmap_mode(\"plot\")\n#> tmap mode set to 'plot'\nlibrary(cartogram)\n#> \n#> Attaching package: 'cartogram'\n#> The following object is masked from 'package:terra':\n#> \n#>     cartogram\n# prepare the data\nus = st_transform(us_states, \"EPSG:2163\")\nus = left_join(us, us_states_df, by = c(\"NAME\" = \"state\"))\n# calculate a poverty rate\nus$poverty_rate = us$poverty_level_15 / us$total_pop_15\n# create a regular map\necm1 = tm_shape(us) +\n  tm_polygons(\"poverty_rate\", fill.legend = tm_legend(title = \"Poverty rate\"))\n# create a cartogram\nus_carto = cartogram_cont(us, \"total_pop_15\")\necm2 = tm_shape(us_carto) + \n  tm_polygons(\"poverty_rate\", fill.legend = tm_legend(title = \"Poverty rate\"))\n# combine two maps\ntmap_arrange(ecm1, ecm2)\nlibrary(geogrid)\n\nhex_cells = calculate_grid(africa, grid_type = \"hexagonal\", seed = 25, learning_rate = 0.03)\nafrica_hex = assign_polygons(africa, hex_cells)\n\nreg_cells = calculate_grid(africa, grid_type = \"regular\", seed = 25, learning_rate = 0.03)\nafrica_reg = assign_polygons(africa, reg_cells)\n\ntgg1 = tm_shape(africa) +\n  tm_polygons(\"pop_growth\", fill.legend = tm_legend(title = \"Population's growth (annual %)\"))\ntgg2 = tm_shape(africa_hex) + \n  tm_polygons(\"pop_growth\", fill.legend = tm_legend(title = \"Population's growth (annual %)\"))\ntgg3 = tm_shape(africa_reg) + \n  tm_polygons(\"pop_growth\", fill.legend = tm_legend(title = \"Population's growth (annual %)\"))\n\ntmap_arrange(tgg1, tgg2, tgg3)"},{"path":"gis.html","id":"gis","chapter":"10 Bridges to GIS software","heading":"10 Bridges to GIS software","text":"E1. Compute global solar irradiation area system.file(\"raster/dem.tif\", package = \"spDataLarge\") March 21 11:00 using r.sun GRASS GIS qgisprocess.E2. Compute catchment area catchment slope system.file(\"raster/dem.tif\", package = \"spDataLarge\") using Rsagacmd.E3. Continue working ndvi_segments object created SAGA GIS section.\nExtract average NDVI values ndvi raster group six clusters using kmeans().\nVisualize results.E4. Attach data(random_points, package = \"spDataLarge\") read system.file(\"raster/dem.tif\", package = \"spDataLarge\") R.\nSelect point randomly random_points find dem pixels can seen point (hint: viewshed can calculated using GRASS GIS).\nVisualize result.\nexample, plot hillshade, digital elevation model, viewshed output, point.\nAdditionally, give mapview try.E5. Use gdalinfo via system call raster file stored disk choice.\nkind information can find ?E6. Use gdalwarp decrease resolution raster file (example, resolution 0.5, change 1). Note: -tr -r flags used exercise.E7. Query Californian highways PostgreSQL/PostGIS database living QGIS Cloud introduced chapter.E8. ndvi.tif raster (system.file(\"raster/ndvi.tif\", package = \"spDataLarge\")) contains NDVI calculated Mongón study area based Landsat data September 22nd, 2000.\nUse rstac, gdalcubes, terra download Sentinel-2 images area \n2020-08-01 2020-10-31, calculate NDVI, compare results ndvi.tif.","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(qgisprocess)\n# enable grass\nqgis_enable_plugins(\"grassprovider\")\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nslope = terrain(dem, \"slope\", unit = \"degrees\")\naspect = terrain(dem, \"aspect\", unit = \"degrees\")\nqgis_algo = qgis_algorithms()\ngrep(\"r.sun\", qgis_algo$algorithm, value = TRUE)\nalg = \"grass7:r.sun.incidout\"\nqgis_show_help(alg)\ndem_sun = qgis_run_algorithm(alg,\n                             elevation = dem, aspect = aspect, slope = slope,\n                             day = 80, time = 11)\ndem_sun\n\n# output global (total) irradiance/irradiation [W.m-2] for given time\ngsi_dem = qgis_as_terra(dem_sun$glob_rad)\nplot(dem)\nplot(gsi_dem)\nlibrary(Rsagacmd)\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nsaga = saga_gis(raster_backend = \"terra\", vector_backend = \"sf\")\nswi = saga$ta_hydrology$saga_wetness_index\ntidy(swi)\nswi_results = swi(dem, area_type = 0, slope_type = 1)\nswi_results_all = rast(swi_results)\nplot(swi_results_all[[\"area\"]])\nplot(swi_results_all[[\"slope\"]])\nlibrary(Rsagacmd)\nsaga = saga_gis(raster_backend = \"terra\", vector_backend = \"sf\")\nndvi = rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))\nsg = saga$imagery_segmentation$seed_generation\n\nndvi_seeds = sg(ndvi, band_width = 2)\nplot(ndvi_seeds$seed_grid)\n\nsrg = saga$imagery_segmentation$seeded_region_growing\nndvi_srg = srg(ndvi_seeds$seed_grid, ndvi, method = 1)\nplot(ndvi_srg$segments)\n\nndvi_segments = as.polygons(ndvi_srg$segments) |> \n  st_as_sf()\n\n# extract values\nndvi_segments_vals = extract(ndvi, ndvi_segments, fun = \"mean\")\nndvi_segments = cbind(ndvi_segments, ndvi_segments_vals)\n\n# k-means\nks = kmeans(ndvi_segments[[\"ndvi\"]], centers = 6)\nndvi_segments$k = ks$cluster\n\n# merge polygons\nlibrary(dplyr)\nndvi_segments2 = ndvi_segments |> \n  group_by(k) |> \n  summarise()\n\n# visualize results\nlibrary(tmap)\ntm1 = tm_shape(ndvi) +\n  tm_raster(style = \"cont\", palette = \"PRGn\", title = \"NDVI\", n = 7) + \n  tm_shape(ndvi_segments2) +\n  tm_borders(col = \"red\") +\n  tm_layout(legend.outside = TRUE)\n\ntm2 = tm_shape(ndvi_segments2) +\n  tm_polygons(col = \"k\", style = \"cat\", palette = \"Set1\") +\n  tm_layout(legend.outside = TRUE)\n\ntmap_arrange(tm1, tm2)\nlibrary(rgrass)\ndem = rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\ndata(random_points, package = \"spDataLarge\")\nrandom_point = random_points[sample(1:nrow(random_points), 1), ]\n\nlink2GI::linkGRASS(dem)\nwrite_RAST(dem, vname = \"dem\")\n\nexecGRASS(\"r.viewshed\",\n          input = \"dem\", \n          coordinates = sf::st_coordinates(random_point),\n          output = \"view\",\n          flags = \"overwrite\")\nout = read_RAST(\"view\")\n\n# simple viz\nplot(out)\n\n# hillshade viz\nhs = shade(slope = terrain(dem, \"slope\", unit = \"radians\"), \n           aspect = terrain(dem, \"aspect\", unit = \"radians\"))\n\nlibrary(tmap)\ntm_shape(hs) +\n    tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) +\n    tm_shape(dem) +\n    tm_raster(alpha = 0.6, palette = hcl.colors(25, \"Geyser\"), legend.show = FALSE) +\n  tm_shape(out) +\n  tm_raster(style = \"cont\", legend.show = FALSE) +\n    tm_shape(random_point) +\n    tm_symbols(col = \"black\") +\n    tm_layout(frame = FALSE)\n\n# mapview viz\nlibrary(mapview)\nmapview(out, col = \"white\", map.type = \"Esri.WorldImagery\") +\n  mapview(point)\nlink2GI::linkGDAL()\nour_filepath = system.file(\"raster/elev.tif\", package = \"spData\")\ncmd = paste(\"gdalinfo\", our_filepath)\nsystem(cmd)\n# Driver, file path, dimensions, CRS, resolution, bounding box, summary statistics\nour_filepath = system.file(\"raster/elev.tif\", package = \"spData\")\ncmd2 = paste(\"gdalwarp\", our_filepath, \"new_elev.tif\", \"-tr 1 1\", \"-r bilinear\")\nsystem(cmd2)\nlibrary(RPostgreSQL)\nconn = dbConnect(drv = PostgreSQL(), \n                 dbname = \"rtafdf_zljbqm\", host = \"db.qgiscloud.com\",\n                 port = \"5432\", user = \"rtafdf_zljbqm\", password = \"d3290ead\")\nquery = paste(\n  \"SELECT *\",\n  \"FROM highways\",\n  \"WHERE state = 'CA';\")\nca_highways = read_sf(conn, query = query, geom = \"wkb_geometry\")\nplot(st_geometry(ca_highways))\nlibrary(rstac)\nlibrary(gdalcubes)\n?spDataLarge::ndvi.tif\nndvi1 = rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))\nbbox1 = as.numeric(st_bbox(project(ndvi1, \"EPSG:4326\")))\n\n# get data\ns = stac(\"https://earth-search.aws.element84.com/v0\")\nitems = s |>\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = bbox1, \n              datetime = \"2020-08-01/2020-10-31\") |>\n  post_request() |> items_fetch()\ncollection = stac_image_collection(items$features, \n                  property_filter = function(x) {x[[\"eo:cloud_cover\"]] < 10})\nv = cube_view(srs = \"EPSG:32717\", extent = collection,\n              dx = xres(ndvi1), dy = yres(ndvi1),\n              dt = \"P1D\")\n\n# calculate ndvi\nndvi2 = raster_cube(collection, v) |>\n  select_bands(c(\"B04\", \"B08\")) |>\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\")\n\n# write results to file\ngdalcubes_options(parallel = 2)\ngdalcubes::write_tif(ndvi2, dir = \".\", prefix = \"ndvi2\")\n\n# unify two datasets\nndvi2 = rast(\"ndvi22020-10-10.tif\")\nplot(ndvi2)\nndvi2 = resample(ndvi2, ndvi1, method = \"bilinear\")\nplot(ndvi2)\n\n# vizualize the final results\nndvi_all = c(ndvi1, ndvi2)\nnames(ndvi_all) = c(\"y2000\", \"y2020\")\nlibrary(tmap)\ntm_shape(ndvi_all) +\n  tm_raster(style = \"cont\")"},{"path":"algorithms.html","id":"algorithms","chapter":"11 Scripts, algorithms and functions","heading":"11 Scripts, algorithms and functions","text":"solutions assume following packages attached (packages attached needed):E1. Read script 11-centroid-alg.R code folder book’s GitHub repo.best practices covered Section ?? follow?Create version script computer IDE RStudio (preferably typing-script line--line, coding style comments, rather copy-pasting — help learn type scripts). Using example square polygon (e.g., created poly_mat = cbind(x = c(0, 9, 9, 0, 0), y = c(0, 0, 9, 9, 0))) execute script line--line.changes made script make reproducible?documentation improved?script stored logical location sensible file name.\nscript well documented comments code well formatted.\nscript reproducible.Open file create new script RStudio, e.g. keyboard shortcut Ctrl + Shift + N (Windows) Cmd + Shift + N (Mac), clicking File > New File > R Script clicking + icon top left Source pane.\ncan also create new R script R console command file.create(\"11-centroid-alg.R\").script already reproducible, message stating needs object called poly_mat present , none present, creates example dataset outset testing.\npeople new R also contain comment stating R must installed running script.Documentation improved detailed description algorithm, including link relevant section book.\nFurthermore, anonymous functions replaced named functions documented Roxygen2 comments.E2. geometric algorithms section calculated area geographic centroid polygon represented poly_mat 245 8.8, 9.2, respectively.Reproduce results computer reference script 11-centroid-alg.R, implementation algorithm (bonus: type commands - try avoid copy-pasting).results correct? Verify converting poly_mat sfc object (named poly_sfc) st_polygon() (hint: function takes objects class list()) using st_area() st_centroid().E3. stated algorithm created works convex hulls. Define convex hulls (see geometry operations chapter) test algorithm polygon convex hull.Bonus 1: Think method works convex hulls note changes need made algorithm make work types polygon.Bonus 2: Building contents 11-centroid-alg.R, write algorithm using base R functions can find total length linestrings represented matrix form.algorithm need able negative well positive area values.leave Bonus 2 exercise reader.E4. functions section created different versions poly_centroid() function generated outputs class sfg (poly_centroid_sfg()) type-stable matrix outputs (poly_centroid_type_stable()).\nextend function creating version (e.g., called poly_centroid_sf()) type stable (accepts inputs class sf) returns sf objects (hint: may need convert object x matrix command sf::st_coordinates(x)).Verify works running poly_centroid_sf(sf::st_sf(sf::st_sfc(poly_sfc)))error message get try run poly_centroid_sf(poly_mat)?","code":"\nlibrary(sf)\n# We can verify the answer by converting `poly_mat` into a simple feature collection\n# as follows, which shows the calculations match:\nx_coords = c(10, 20, 12, 0, 0, 10)\ny_coords = c(0, 15, 20, 10, 0, 0)\npoly_mat = cbind(x_coords, y_coords)\npoly_sfc = sf::st_polygon(list(poly_mat))\nsf::st_area(poly_sfc)\nsf::st_centroid(poly_sfc)\n# By calling the script:\n# source(\"https://github.com/geocompx/geocompr/raw/main/code/11-centroid-alg.R\")\nx_coords = c(10, 20, 12, 0, 0, 5, 10)\ny_coords = c(0, 15, 20, 10, 0, 5, 0)\nplot(x_coords, y_coords, type = \"l\")\npoly_mat = cbind(x_coords, y_coords)\n# source(\"https://github.com/geocompx/geocompr/raw/main/code/11-centroid-alg.R\")\n# Area from our script: 270\npoly_sfc = sf::st_polygon(list(poly_mat))\nsf::st_area(poly_sfc) # Actual area: 220\npoly_centroid_sf = function(x) {\n  stopifnot(is(x, \"sf\"))\n  xcoords = sf::st_coordinates(x)\n  centroid_coords = poly_centroid(xcoords)\n  centroid_sf = sf::st_sf(geometry = sf::st_sfc(sf::st_point(centroid_coords)))\n  centroid_sf\n}\npoly_centroid_sf(sf::st_sf(sf::st_sfc(poly_sfc)))\npoly_centroid_sf(poly_sfc)\npoly_centroid_sf(poly_mat)"},{"path":"spatial-cv.html","id":"spatial-cv","chapter":"12 Statistical learning","heading":"12 Statistical learning","text":"solutions assume following packages attached (packages attached needed):E1. Compute following terrain attributes elev dataset loaded terra::rast(system.file(\"raster/ta.tif\", package = \"spDataLarge\"))$elev help R-GIS bridges (see bridges GIS software chapter):SlopePlan curvatureProfile curvatureCatchment areaE2. Extract values corresponding output rasters lsl data frame (data(\"lsl\", package = \"spDataLarge\") adding new variables called slope, cplan, cprof, elev log_carea.E3. Use derived terrain attribute rasters combination GLM make spatial prediction map similar shown Figure 12.2.\nRunning data(\"study_mask\", package = \"spDataLarge\") attaches mask study area.E4. Compute 100-repeated 5-fold non-spatial cross-validation spatial CV based GLM learner compare AUROC values resampling strategies help boxplots.Hint: need specify non-spatial resampling strategy.Another hint: might want solve Excercises 4 6 one go help mlr3::benchmark() mlr3::benchmark_grid() (information, please refer https://mlr3book.mlr-org.com/performance.html#benchmarking).\n, keep mind computation can take long, probably several days.\n, course, depends system.\nComputation time shorter RAM cores disposal.E5. Model landslide susceptibility using quadratic discriminant analysis (QDA).\nAssess predictive performance QDA.\ndifference spatially cross-validated mean AUROC value QDA GLM?E6. Run SVM without tuning hyperparameters.\nUse rbfdot kernel \\(\\sigma\\) = 1 C = 1.\nLeaving hyperparameters unspecified kernlab’s ksvm() otherwise initialize automatic non-spatial hyperparameter tuning.","code":"\nlibrary(dplyr)\n# library(kernlab)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3extralearners)\nlibrary(mlr3spatiotempcv)\nlibrary(mlr3tuning)\nlibrary(qgisprocess)\nlibrary(terra)\nlibrary(sf)\nlibrary(tmap)\n# attach data\ndem = terra::rast(system.file(\"raster/ta.tif\", package = \"spDataLarge\"))$elev\n\nalgs = qgisprocess::qgis_algorithms()\nqgis_search_algorithms(\"curvature\")\nalg = \"sagang:slopeaspectcurvature\"\nqgisprocess::qgis_show_help(alg)\nqgisprocess::qgis_get_argument_specs(alg)\n# terrain attributes (ta)\nout_nms = paste0(tempdir(), \"/\", c(\"slope\", \"cplan\", \"cprof\"),\n                 \".sdat\")\nargs = rlang::set_names(out_nms, c(\"SLOPE\", \"C_PLAN\", \"C_PROF\"))\nout = qgis_run_algorithm(alg, ELEVATION = dem, METHOD = 6, \n                         UNIT_SLOPE = \"[1] degree\",\n                         !!!args,\n                         .quiet = TRUE\n                         )\nta = out[names(args)] |> unlist() |> terra::rast()\nnames(ta) = c(\"slope\", \"cplan\", \"cprof\")\n# catchment area\nqgis_search_algorithms(\"[Cc]atchment\")\nalg = \"sagang:catchmentarea\"\nqgis_show_help(alg)\nqgis_get_argument_specs(alg)\ncarea = qgis_run_algorithm(alg,\n                           ELEVATION = dem, \n                           METHOD = 4, \n                           FLOW = file.path(tempdir(), \"carea.sdat\"))\n# transform carea\ncarea = terra::rast(carea$FLOW[1])\nlog10_carea = log10(carea)\nnames(log10_carea) = \"log10_carea\"\n# add log_carea and dem to the terrain attributes\nta = c(ta, dem, log10_carea)\n# attach terrain attribute raster stack (in case you have skipped the previous\n# exercise)\ndata(\"lsl\", package = \"spDataLarge\")\nta = terra::rast(system.file(\"raster/ta.tif\", package = \"spDataLarge\"))\nlsl = select(lsl, x, y, lslpts)\n# extract values to points, i.e., create predictors\nlsl[, names(ta)] = terra::extract(ta, lsl[, c(\"x\", \"y\")]) |>\n  select(-ID)\n# attach data (in case you have skipped exercises 1) and 2)\n# landslide points with terrain attributes and terrain attribute raster stack\ndata(\"lsl\", \"study_mask\", package = \"spDataLarge\")\nta = terra::rast(system.file(\"raster/ta.tif\", package = \"spDataLarge\"))\n\n# fit the model\nfit = glm(lslpts ~ slope + cplan + cprof + elev + log10_carea, \n          data = lsl, family = binomial())\n\n# make the prediction\npred = terra::predict(object = ta, model = fit, type = \"response\")\n\n# make the map\nlsl_sf = sf::st_as_sf(lsl, coords = c(\"x\", \"y\"), crs = 32717)\nlsl_sf = sf::st_as_sf(lsl, coords = c(\"x\", \"y\"), crs = 32717)\nhs = terra::shade(ta$slope * pi / 180,\n                  terra::terrain(ta$elev, v = \"aspect\", unit = \"radians\"))\nrect = tmaptools::bb_poly(raster::raster(hs))\nbbx = tmaptools::bb(raster::raster(hs), xlim = c(-0.00001, 1),\n                    ylim = c(-0.00001, 1), relative = TRUE)\n\ntm_shape(terra::mask(hs, study_mask), bbox = bbx) +\n  tm_grid(col = \"black\", n.x = 1, n.y = 1, labels.inside.frame = FALSE,\n          labels.rot = c(0, 90), lines = FALSE) +\n    tm_raster(col.scale = tm_scale(values = gray(0:100 / 100), n = 100),\n            col.legend = tm_legend_hide()) +\n    # prediction raster\n  tm_shape(terra::mask(pred, study_mask)) +\n    tm_raster(col_alpha = 0.5,\n            col.scale = tm_scale(values = \"Reds\", n = 6),\n            col.legend = tm_legend(title = \"Susceptibility\")) +\n    # rectangle and outer margins\n  tm_shape(rect) + \n  tm_borders() +\n    tm_layout(legend.position = c(\"left\", \"bottom\"),\n              legend.title.size = 0.9)\n# attach data (in case you have skipped exercises 1) and 2)\ndata(\"lsl\", package = \"spDataLarge\")  # landslide points with terrain attributes\n\n# create task\ntask = TaskClassifST$new(\n  id = \"lsl_ecuador\",\n  backend = mlr3::as_data_backend(lsl), target = \"lslpts\", positive = \"TRUE\",\n  coordinate_names = c(\"x\", \"y\"),\n  coords_as_features = FALSE,\n  crs = 32717\n)\n\n# construct learners (for all subsequent exercises)\n# GLM\nlrn_glm = lrn(\"classif.log_reg\", predict_type = \"prob\")\nlrn_glm$fallback = lrn(\"classif.featureless\", predict_type = \"prob\")\n\n# SVM\n# construct SVM learner (using ksvm function from the kernlab package)\nlrn_ksvm = lrn(\"classif.ksvm\", predict_type = \"prob\", kernel = \"rbfdot\",\n               type = \"C-svc\")\nlrn_ksvm$fallback = lrn(\"classif.featureless\", predict_type = \"prob\")\n\n# specify nested resampling and adjust learner accordingly\n# five spatially disjoint partitions\ntune_level = rsmp(\"spcv_coords\", folds = 5)\n# use 50 randomly selected hyperparameters\nterminator = trm(\"evals\", n_evals = 50)\ntuner = tnr(\"random_search\")\n# define the outer limits of the randomly selected hyperparameters\nps = ps(\n  C = p_dbl(lower = -12, upper = 15, trafo = function(x) 2^x),\n  sigma = p_dbl(lower = -15, upper = 6, trafo = function(x) 2^x)\n)\nat_ksvm = AutoTuner$new(\n  learner = lrn_ksvm,\n  resampling = tune_level,\n  measure = msr(\"classif.auc\"),\n  search_space = ps,\n  terminator = terminator,\n  tuner = tuner\n)\n\n# QDA\nlrn_qda = lrn(\"classif.qda\", predict_type = \"prob\")\nlrn_qda$fallback = lrn(\"classif.featureless\", predict_type = \"prob\")\n\n# SVM without tuning hyperparameters\nvals = lrn_ksvm$param_set$values\nlrn_ksvm_notune = lrn_ksvm$clone()\nlrn_ksvm_notune$param_set$values = c(vals, C = 1, sigma = 1)\n\n# define resampling strategies\n# specify the reampling method, i.e. spatial CV with 100 repetitions and 5 folds\n# -> in each repetition dataset will be splitted into five folds\n# method: repeated_spcv_coords -> spatial partioning\nrsmp_sp = rsmp(\"repeated_spcv_coords\", folds = 5, repeats = 100)\n# method: repeated_cv -> non-spatial partitioning\nrsmp_nsp = rsmp(\"repeated_cv\", folds = 5, repeats = 100)\n\n# (spatial) cross-validataion\n#****************************\n# create your design\ngrid = benchmark_grid(tasks = task, \n                      learners = list(lrn_glm, at_ksvm, lrn_qda, \n                                      lrn_ksvm_notune),\n                      resamplings = list(rsmp_sp, rsmp_nsp))\n# execute the cross-validation\nlibrary(future)\n# execute the outer loop sequentially and parallelize the inner loop\nfuture::plan(list(\"sequential\", \"multisession\"), \n             workers = floor(availableCores() / 2))\nset.seed(021522)\nbmr = benchmark(grid, \n                store_backends = FALSE, \n                store_models = FALSE, \n                encapsulate = \"evaluate\")\n# stop parallelization\nfuture:::ClusterRegistry(\"stop\")\n# save your result, e.g. to \n# saveRDS(bmr, file = \"extdata/12-bmr.rds\")\n\n# plot your result\nautoplot(bmr, measure = msr(\"classif.auc\"))\n# attach data (in case you have skipped exercise 4)\nbmr = readRDS(\"extdata/12-bmr.rds\")\n\n# plot your result\nautoplot(bmr, measure = msr(\"classif.auc\"))\n# QDA has higher AUROC values on average than GLM which indicates moderately\n# non-linear boundaries\n# attach data (in case you have skipped exercise 4)\nbmr = readRDS(\"extdata/12-bmr.rds\")\n# plot your result\nautoplot(bmr, measure = msr(\"classif.auc\"))"},{"path":"transport.html","id":"transport","chapter":"13 Transportation","heading":"13 Transportation","text":"E1. much analysis presented chapter focused active modes, driving trips?proportion trips desire_lines object made driving?proportion desire_lines straight line length 5 km distance?proportion trips desire lines longer 5 km length made driving?Plot desire lines less 5 km length along 50% trips made car.notice location car dependent yet short desire lines?E2. additional length cycleways result routes presented last Figure, sections beyond 100 m existing cycleways, constructed?E3. proportion trips represented desire_lines accounted routes_short_scenario object?Bonus: proportion trips happen desire lines cross routes_short_scenario?E4. analysis presented chapter designed teaching geocomputation methods can applied transport research.\nreal, government transport consultancy, top 3 things differently?E5. Clearly, routes identified last Figure provide part picture.\nextend analysis?E6. Imagine want extend scenario creating key areas (routes) investment place-based cycling policies car-free zones, cycle parking points reduced car parking strategy.\nraster datasets assist work?Bonus: develop raster layer divides Bristol region 100 cells (10 10) estimate average speed limit roads , bristol_ways dataset (see Chapter 14).","code":"\nlibrary(sf)\nlibrary(spDataLarge)\n# Higher level of geographic resolution.\n# Use cycle-specific routing services.\n# Identify key walking routes.\n# Include a higher proportion of trips in the analysis"},{"path":"location.html","id":"location","chapter":"14 Geomarketing","heading":"14 Geomarketing","text":"solutions assume following packages attached (packages attached needed):E1. Download csv file containing inhabitant information 100 m cell resolution (https://www.zensus2011.de/SharedDocs/Downloads/DE/Pressemitteilung/DemografischeGrunddaten/csv_Bevoelkerung_100m_Gitter.zip?__blob=publicationFile&v=3).\nPlease note unzipped file size 1.23 GB.\nread R can use readr::read_csv.\ntakes 30 seconds machine 16 GB RAM.\ndata.table::fread() might even faster, returns object class data.table().\nUse dplyr::as_tibble() convert tibble.\nBuild inhabitant raster, aggregate cell resolution 1 km, compare difference inhabitant raster (inh) created using class mean values.E2. Suppose bike shop predominantly sold electric bikes older people.\nChange age raster accordingly, repeat remaining analyses compare changes original result.","code":"\nlibrary(sf)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(terra)\nlibrary(osmdata)\nlibrary(spDataLarge)\n# Coarse inhabitant raster (1 km resolution)\n#*******************************************\n\n# inhabitant raster (coarse resolution); this is one of the results of the \n# previous exercise\ndata(\"census_de\", package = \"spDataLarge\")\ninput = select(census_de, x = x_mp_1km, y = y_mp_1km, pop = Einwohner,\n                      women = Frauen_A, mean_age = Alter_D, hh_size = HHGroesse_D)\ninput_tidy = dplyr::mutate(input, dplyr::across(.fns = ~ifelse(. %in% c(-1, -9), NA, .)))\ninput_ras = terra::rast(input_tidy, type = \"xyz\", crs = \"EPSG:3035\")\ninh_coarse = input_ras$pop\n# reclassify, i.e. convert the classes into inhabitant numbers using class means\nrcl = matrix(c(1, 1, 125, 2, 2, 375, 3, 3, 1250, 4, 4, 3000, 5, 5, 6000,\n               6, 6, 8000), ncol = 3, byrow = TRUE)\ninh_coarse = terra::classify(inh_coarse, rcl = rcl, right = NA)\n\n# Fine inhabitant raster (100 m resolution)\n#******************************************\nurl =\n  paste0(\"https://www.zensus2011.de/SharedDocs/Downloads/DE/Pressemitteilung/\",\n         \"DemografischeGrunddaten/csv_Bevoelkerung_100m_Gitter.zip\", \n         \"?__blob=publicationFile&v=3\")\n# download fine raster\ndownload.file(url = url, destfile = file.path(tempdir(), \"census.zip\"),\n              method = \"auto\", mode = \"wb\")\n# list the file names\nnms = unzip(file.path(tempdir(), \"census.zip\"), list = TRUE)\n# unzip only the csv file\nbase_name = grep(\".csv$\", nms$Name, value = TRUE)\nunzip(file.path(tempdir(), \"census.zip\"), files = base_name, exdir = tempdir())\n# read in the csv file\ninput = data.table::fread(file.path(tempdir(), base_name)) |>\n  dplyr::as_tibble()\ninput = select(input, x = starts_with(\"x_mp_1\"),\n                      y = starts_with(\"y_mp_1\"), inh = Einwohner)\n# set -1 and -9 to NA\ninput = dplyr::mutate(input,\n                      dplyr::across(.fns = ~ifelse(. %in% c(-1, -9), NA, .)))\n# convert table into a raster (x and y are cell midpoints)\ninh_fine = terra::rast(input, type = \"xyz\", crs = \"EPSG:3035\")\n# Note that inh_fine contains the actual number of inhabitants per raster cell\n# instead of mean class values as was the case with its coarse 1km counterpart\n\n# Comparing the coarse with the fine raster\n#******************************************\n\n# aggregate to the resolution of the coarse raster\ninh_fine = terra::aggregate(\n  inh_fine, fact = terra::res(inh_coarse)[1] / terra::res(inh_fine)[1], \n  fun = sum, na.rm = TRUE)\n# origin has to be the same\nterra::origin(inh_fine) = terra::origin(inh_coarse)\n# make the comparison\nsummary(inh_fine - inh_coarse)\nplot(inh_fine - inh_coarse)\nplot(abs(inh_fine - inh_coarse) > 1000)\n# the biggest deviations can be found in big cities like Berlin\nterra::global((abs(inh_fine - inh_coarse) > 1000), fun = \"sum\", na.rm = TRUE)\n# 18,121 cells have a deviation > 1000 inhabitants\nterra::global((abs(inh_fine - inh_coarse) > 5000), fun = \"sum\", na.rm = TRUE)\n# 338 cells have a deviation > 5000\n# Here, we assue that you have already created `input_ras` in the first exercise.\n# attach further necessary data\ndata(\"metro_names\", \"shops\", package = \"spDataLarge\")\n\n# Basically, we are assuming that especially older people will use an electric\n# bike, therefore, we increase the weights for raster cells where predominantly\n# older people are living.\nrcl_pop = matrix(c(1, 1, 127, 2, 2, 375, 3, 3, 1250, \n                   4, 4, 3000, 5, 5, 6000, 6, 6, 8000), \n                 ncol = 3, byrow = TRUE)\nrcl_women = matrix(c(1, 1, 3, 2, 2, 2, 3, 3, 1, 4, 5, 0), \n                   ncol = 3, byrow = TRUE)\n# here we are giving the classes (3 to 5) containing the oldest people the\n# highest weight\nrcl_age = matrix(c(1, 1, 1, 2, 2, 1, 3, 5, 3),\n                 ncol = 3, byrow = TRUE)\nrcl_hh = rcl_women\nrcl = list(rcl_pop, rcl_women, rcl_age, rcl_hh)\n\nreclass = input_ras\nfor (i in 1:terra::nlyr(reclass)) {\n  reclass[[i]] = terra::classify(x = reclass[[i]], rcl = rcl[[i]], right = NA)\n}\nnames(reclass) = names(input_ras)\n\n# The rest of the analysis follows exactly the code presented in the book. \n\n# Add metro names to metros sf object\n#************************************\nmetro_names = dplyr::pull(metro_names, city) |>\n  as.character() |>\n  {\\(x) ifelse(x == \"Velbert\", \"Düsseldorf\", x)}() |>\n  {\\(x) gsub(\"ü\", \"ue\", x)}()\n\npop_agg = terra::aggregate(reclass$pop, fact = 20, fun = sum, na.rm = TRUE)\npop_agg = pop_agg[pop_agg > 500000, drop = FALSE] \n\npolys = pop_agg |>\n  terra::patches(directions = 8) |>\n  terra::as.polygons() |>\n  sf::st_as_sf()\n\nmetros = polys |>\n  dplyr::group_by(patches) |>\n  dplyr::summarize()\nmetros$metro_names = metro_names\n\n# Create shop/poi density raster\n#*******************************\nshops = sf::st_transform(shops, sf::st_crs(reclass))\n# create poi raster\npoi = terra::rasterize(x = shops, y = reclass, field = \"osm_id\", fun = \"length\")\n# construct reclassification matrix\nint = classInt::classIntervals(values(poi), n = 4, style = \"fisher\")\nint = round(int$brks)\nrcl_poi = matrix(c(int[1], rep(int[-c(1, length(int))], each = 2), \n                   int[length(int)] + 1), ncol = 2, byrow = TRUE)\nrcl_poi = cbind(rcl_poi, 0:3)  \n# reclassify\npoi = terra::classify(poi, rcl = rcl_poi, right = NA) \nnames(poi) = \"poi\"\n# remove population raster and add poi raster\nreclass = reclass[[names(reclass) != \"pop\"]] |>\n  c(poi)\n\n# Identify suitable locations\n#****************************\n# calculate the total score\nresult = sum(reclass)\n\n# have a look at suitable bike shop locations in Berlin\nberlin = metros[metro_names == \"Berlin\", ]\nberlin_raster = terra::crop(result, berlin)\n# summary(berlin_raster)\n# berlin_raster\nberlin_raster = berlin_raster > 9\nberlin_raster[berlin_raster == 0] = NA\n# make the plot\nleaflet::leaflet() |>\n  leaflet::addTiles() |>\n  leaflet::addRasterImage(raster::raster(berlin_raster), colors = \"darkgreen\", opacity = 0.8) |>\n  leaflet::addLegend(\"bottomright\", colors = c(\"darkgreen\"), \n                     labels = c(\"potential locations\"), title = \"Legend\")"},{"path":"eco.html","id":"eco","chapter":"15 Ecology","heading":"15 Ecology","text":"solutions assume following packages attached (packages attached needed):E1. Run NMDS using percentage data community matrix.\nReport stress value compare stress value retrieved NMDS using presence-absence data.\nmight explain observed difference?NMDS using presence-absence values yields better result (nmds_pa$stress) one using percentage data (nmds_per$stress).\nmight seem surprising first sight.\nhand, percentage matrix contains information noise.\nAnother aspect data collected.\nImagine botanist field.\nmight seem feasible differentiate plant cover 5% another species covers 10%.\nHowever, herbal species detected three times consequently tiny cover, e.g., 0.0001%.\nMaybe another herbal species detected 6 times, cover 0.0002%?\npoint percentage data specified field campaign might reflect precision data .\nintroduces noise turn worsen ordination result.\nStill, valuable information one species higher frequency coverage one plot another compared just presence-absence data.\nOne compromise use categorical scale Londo scale.E2. Compute predictor rasters used chapter (catchment slope, catchment area), put SpatRaster-object.\nAdd dem ndvi .\nNext, compute profile tangential curvature add additional predictor rasters (hint: grass7:r.slope.aspect).\nFinally, construct response-predictor matrix.\nscores first NMDS axis (result using presence-absence community matrix) rotated accordance elevation represent response variable, joined random_points (use inner join).\ncomplete response-predictor matrix, extract values environmental predictor raster object random_points.E3. Retrieve bias-reduced RMSE random forest linear model using spatial cross-validation.\nrandom forest modeling include estimation optimal hyperparameter combinations (random search 50 iterations) inner tuning loop.\nParallelize tuning level.\nReport mean RMSE use boxplot visualize retrieved RMSEs.\nPlease exercise best solved using mlr3 functions benchmark_grid() benchmark() (see https://mlr3book.mlr-org.com/perf-eval-cmp.html#benchmarking information).fact, lm performs least good random forest model, thus preferred since much easier understand computationally much less demanding (need fitting hyperparameters).\nkeep mind used dataset small terms observations predictors response-predictor relationships also relatively linear.","code":"\nlibrary(sf)\nlibrary(terra)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(future)\nlibrary(ggplot2)\nlibrary(lgr)\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3spatiotempcv)\nlibrary(mlr3tuning)\nlibrary(mlr3viz)\nlibrary(progressr)\nlibrary(qgisprocess)\nlibrary(tictoc)\nlibrary(vegan)\ndata(\"comm\", package = \"spDataLarge\")\npa = vegan::decostand(comm, \"pa\")\npa = pa[rowSums(pa) != 0, ]\ncomm = comm[rowSums(comm) != 0, ]\nset.seed(25072018)\nnmds_pa = vegan::metaMDS(comm = pa, k = 4, try = 500)\nnmds_per = vegan::metaMDS(comm = comm, k = 4, try = 500)\nnmds_pa$stress\nnmds_per$stress\n# first compute the terrain attributes we have also used in the chapter\nlibrary(dplyr)\nlibrary(terra)\nlibrary(qgisprocess)\nlibrary(vegan)\ndata(\"comm\", \"random_points\", package = \"spDataLarge\")\ndem = terra::rast(system.file(\"raster/dem.tif\", package = \"spDataLarge\"))\nndvi = terra::rast(system.file(\"raster/ndvi.tif\", package = \"spDataLarge\"))\n\n# use presence-absence matrix and get rid of empty\npa = vegan::decostand(comm, \"pa\")\npa = pa[rowSums(pa) != 0, ]\n\n# enable plugins if not already done so\nqgisprocess::qgis_enable_plugins(c(\"grassprovider\", \"processing_saga_nextgen\"))\n\n# compute environmental predictors (ep) catchment slope and catchment area\nep = qgisprocess::qgis_run_algorithm(\n  alg = \"sagang:sagawetnessindex\",\n  DEM = dem,\n  SLOPE_TYPE = 1,\n  SLOPE = tempfile(fileext = \".sdat\"),\n  AREA = tempfile(fileext = \".sdat\"),\n  .quiet = TRUE)\n# read in catchment area and catchment slope\nep = ep[c(\"AREA\", \"SLOPE\")] |>\n  unlist() |>\n  terra::rast()\n# assign proper names \nnames(ep) = c(\"carea\", \"cslope\")\n# make sure all rasters share the same origin\norigin(ep) = origin(dem)\n# add dem and ndvi to the multilayer SpatRaster object\nep = c(dem, ndvi, ep) \nep$carea = log10(ep$carea)\n\n# compute the curvatures\nqgis_show_help(\"grass7:r.slope.aspect\")\ncurvs = qgis_run_algorithm(\n  \"grass7:r.slope.aspect\",\n  elevation = dem,\n  .quiet = TRUE)\n# adding curvatures to ep\ncurv_nms = c(\"pcurvature\", \"tcurvature\")\ncurvs = curvs[curv_nms] |>\n  unlist() |>\n  terra::rast()\ncurvs = terra::app(curvs, as.numeric)\nnames(curvs) = curv_nms\nep = c(ep, curvs)\nrandom_points[, names(ep)] = \n  # terra::extract adds an ID column, we don't need\n  terra::extract(ep, random_points) |>\n  select(-ID)\nelev = dplyr::filter(random_points, id %in% rownames(pa)) %>% \n  dplyr::pull(dem)\n# rotating NMDS in accordance with altitude (proxy for humidity)\nrotnmds = MDSrotate(nmds_pa, elev)\n# extracting the first two axes\nsc = vegan::scores(rotnmds, choices = 1:2, display = \"sites\")\nrp = data.frame(id = as.numeric(rownames(sc)),\n                sc = sc[, 1])\n# join the predictors (dem, ndvi and terrain attributes)\nrp = dplyr::inner_join(random_points, rp, by = \"id\")\n# saveRDS(rp, \"extdata/15-rp_exercises.rds\")\nlibrary(dplyr)\nlibrary(future)\nlibrary(mlr3)\nlibrary(mlr3spatiotempcv)\nlibrary(mlr3learners)\nlibrary(mlr3viz)\nlibrary(paradox)\nlibrary(ranger)\n\n# in case you have not run the previous exercises, run:\n# rp = readRDS(\"extdata/15-rp_exercises.rds\")\n\n# define the task\ntask = mlr3spatiotempcv::as_task_regr_st(\n  select(rp, -id, -spri),\n  target = \"sc\", \n  id = \"mongon\")\n\n# define the learners\nmlr3::mlr_learners\n# linear model\nlrn_lm = mlr3::lrn(\"regr.lm\", predict_type = \"response\")\n# random forest\nlrn_rf = mlr3::lrn(\"regr.ranger\", predict_type = \"response\")\n# now define the AutoTuner of the random forest\nsearch_space = paradox::ps(\n  mtry = paradox::p_int(lower = 1, upper = ncol(task$data()) - 1),\n  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),\n  min.node.size = paradox::p_int(lower = 1, upper = 10)\n)\nat_rf = mlr3tuning::AutoTuner$new(\n  learner = lrn_rf,\n  # spatial partitioning\n  resampling = mlr3::rsmp(\"spcv_coords\", folds = 5),\n  # performance measure\n  measure = mlr3::msr(\"regr.rmse\"),\n  search_space = search_space,\n  # random search with 50 iterations\n  terminator = mlr3tuning::trm(\"evals\", n_evals = 50),\n  tuner = mlr3tuning::tnr(\"random_search\")\n)\n# define the resampling strategy\nrsmp_sp = mlr3::rsmp(\"repeated_spcv_coords\", folds = 5, repeats = 100)\n\n# create the benchmark design\ndesign_grid = mlr3::benchmark_grid(\n  tasks = task,\n  learners = list(lrn_lm, at_rf),\n  resamplings = rsmp_sp)\nprint(design_grid)\n# execute the outer loop sequentially and parallelize the inner loop\nfuture::plan(list(\"sequential\", \"multisession\"), \n             workers = floor(future::availableCores() / 2))\nset.seed(10112022)\n# reduce verbosity\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"info\")\n# BE CAREFUL: Running the benchmark might take quite some time\n# therefore, we have stored the result of the benchmarking in\n# extdata/15-bmr-exercises.rds (see below)\ntictoc::tic()\nprogressr::with_progress(expr = {\n  bmr = mlr3::benchmark(\n    design = design_grid,\n    # New argument `encapsulate` for `resample()` and `benchmark()` to\n    # conveniently enable encapsulation and also set the fallback learner to the\n    # respective featureless learner. This is simply for convenience, configuring\n    # each learner individually is still possible and allows a more fine-grained\n    # control\n    encapsulate = \"evaluate\",\n    store_backends = FALSE,\n    store_models = FALSE)\n})\ntictoc::toc()\n\n# stop parallelization\nfuture:::ClusterRegistry(\"stop\")\n# we have saved the result as follows\n# saveRDS(bmr, file = \"extdata/15-bmr_exercises.rds\")\n# READ IT IN in in case you don't want to run the spatial CV yourself as it is computationally\n# demanding\n# bmr = readRDS(\"extdata/15-bmr_exercises.rds\")\n\n# mean RMSE\nbmr$aggregate(measures = msr(\"regr.rmse\"))\n# or computed manually\nagg = bmr$aggregate(measures = msr(\"regr.rmse\"))\n\n# lm performs slightly better when considering the mean rmse\npurrr::map(agg$resample_result, ~ mean(.$score(msr(\"regr.rmse\"))$regr.rmse))\n# however, looking at the median, the random forest performs slightly better\npurrr::map(agg$resample_result, ~ median(.$score(msr(\"regr.rmse\"))$regr.rmse))\n\n# make a boxplot (when using autoplot, mlr3viz needs to be attached!)\nlibrary(mlr3viz)\nautoplot(bmr, measure = msr(\"regr.rmse\"))\n\n# or doing it \"manually\"\n# extract the AUROC values and put them into one data.table\nd = purrr::map_dfr(agg$resample_result, ~ .$score(msr(\"regr.rmse\")))\n# create the boxplots\nlibrary(ggplot2)\nggplot(data = d, mapping = aes(x = learner_id, y = regr.rmse)) +\n  geom_boxplot(fill = c(\"lightblue2\", \"mistyrose2\")) +\n  theme_bw() +\n  labs(y = \"RMSE\", x = \"model\")"}]
